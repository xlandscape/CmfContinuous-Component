.\" Man page generated from reStructuredText.
.
.TH "NUMDIFFTOOLS" "1" "September 08, 2016" "0.9.17" "numdifftools"
.SH NAME
numdifftools \- numdifftools 0.9.17
.
.nr rst2man-indent-level 0
.
.de1 rstReportMargin
\\$1 \\n[an-margin]
level \\n[rst2man-indent-level]
level margin: \\n[rst2man-indent\\n[rst2man-indent-level]]
-
\\n[rst2man-indent0]
\\n[rst2man-indent1]
\\n[rst2man-indent2]
..
.de1 INDENT
.\" .rstReportMargin pre:
. RS \\$1
. nr rst2man-indent\\n[rst2man-indent-level] \\n[an-margin]
. nr rst2man-indent-level +1
.\" .rstReportMargin post:
..
.de UNINDENT
. RE
.\" indent \\n[an-margin]
.\" old: \\n[rst2man-indent\\n[rst2man-indent-level]]
.nr rst2man-indent-level -1
.\" new: \\n[rst2man-indent\\n[rst2man-indent-level]]
.in \\n[rst2man-indent\\n[rst2man-indent-level]]u
..
\fI\%\fP\fI\%\fP\fI\%\fP\fI\%Code Health\fP\fI\%\fP\fI\%\fP
.sp
This is the documentation of \fBNumdifftools\fP 0.9.17\&.
.sp
Bleeding edge available at: \fI\%https://github.com/pbrod/numdifftools\fP\&.
.sp
Official releases are available at: \fI\%http://pypi.python.org/pypi/Numdifftools\fP\&.
.SH INTRODUCTION TO NUMDIFFTOOLS
.sp
\fI\%pkg_img\fP \fI\%tests_img\fP \fI\%tests2_img\fP \fI\%docs_img\fP \fI\%Code Health\fP \fI\%coverage_img\fP \fI\%versions_img\fP \fI\%depsy_img\fP
.sp
Numdifftools is a suite of tools written in \fI\%_Python\fP
to solve automatic numerical differentiation problems in one or more variables.
Finite differences are used in an adaptive manner, coupled with a Richardson
extrapolation methodology to provide a maximally accurate result.
The user can configure many options like; changing the order of the method or
the extrapolation, even allowing the user to specify whether complex\-step, central,
forward or backward differences are used.
.sp
The methods provided are:
.INDENT 0.0
.IP \(bu 2
\fBDerivative\fP: Compute the derivatives of order 1 through 10 on any scalar function.
.IP \(bu 2
\fBdirectionaldiff\fP: Compute directional derivative of a function of n variables
.IP \(bu 2
\fBGradient\fP: Compute the gradient vector of a scalar function of one or more variables.
.IP \(bu 2
\fBJacobian\fP: Compute the Jacobian matrix of a vector valued function of one or more variables.
.IP \(bu 2
\fBHessian\fP: Compute the Hessian matrix of all 2nd partial derivatives of a scalar function of one or more variables.
.IP \(bu 2
\fBHessdiag\fP: Compute only the diagonal elements of the Hessian matrix
.UNINDENT
.sp
All of these methods also produce error estimates on the result.
.sp
Numdifftools also provide an easy to use interface to derivatives calculated
with in \fI\%_AlgoPy\fP\&. Algopy stands for Algorithmic
Differentiation in Python.
The purpose of AlgoPy is the evaluation of higher\-order derivatives in the
\fIforward\fP and \fIreverse\fP mode of Algorithmic Differentiation (AD) of functions
that are implemented as Python programs.
.SS Getting Started
.sp
Visualize high order derivatives of the tanh function
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> import numpy as np
>>> import numdifftools as nd
>>> import matplotlib.pyplot as plt
>>> x = np.linspace(\-2, 2, 100)
>>> for i in range(10):
\&...    df = nd.Derivative(np.tanh, n=i)
\&...    y = df(x)
\&...    h = plt.plot(x, y/np.abs(y).max())
.ft P
.fi
.sp
plt.show()
.UNINDENT
.UNINDENT
\fI\%\fP
.sp
Compute 1\(aqst and 2\(aqnd derivative of exp(x), at x == 1:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> fd = nd.Derivative(np.exp)        # 1\(aqst derivative
>>> fdd = nd.Derivative(np.exp, n=2)  # 2\(aqnd derivative
>>> np.allclose(fd(1), 2.7182818284590424)
True
>>> np.allclose(fdd(1), 2.7182818284590424)
True
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Nonlinear least squares:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> xdata = np.reshape(np.arange(0,1,0.1),(\-1,1))
>>> ydata = 1+2*np.exp(0.75*xdata)
>>> fun = lambda c: (c[0]+c[1]*np.exp(c[2]*xdata) \- ydata)**2
>>> Jfun = nd.Jacobian(fun)
>>> np.allclose(np.abs(Jfun([1,2,0.75])), 0) # should be numerically zero
True
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Compute gradient of sum(x**2):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> fun = lambda x: np.sum(x**2)
>>> dfun = nd.Gradient(fun)
>>> dfun([1,2,3])
array([ 2.,  4.,  6.])
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Compute the same with the easy to use interface to AlgoPy:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> import numdifftools.nd_algopy as nda
>>> import numpy as np
>>> fd = nda.Derivative(np.exp)        # 1\(aqst derivative
>>> fdd = nda.Derivative(np.exp, n=2)  # 2\(aqnd derivative
>>> np.allclose(fd(1), 2.7182818284590424)
True
>>> np.allclose(fdd(1), 2.7182818284590424)
True
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Nonlinear least squares:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> xdata = np.reshape(np.arange(0,1,0.1),(\-1,1))
>>> ydata = 1+2*np.exp(0.75*xdata)
>>> fun = lambda c: (c[0]+c[1]*np.exp(c[2]*xdata) \- ydata)**2
>>> Jfun = nda.Jacobian(fun, method=\(aqreverse\(aq)
>>> np.allclose(np.abs(Jfun([1,2,0.75])), 0) # should be numerically zero
True
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Compute gradient of sum(x**2):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> fun = lambda x: np.sum(x**2)
>>> dfun = nda.Gradient(fun)
>>> dfun([1,2,3])
array([ 2.,  4.,  6.])
.ft P
.fi
.UNINDENT
.UNINDENT
.SS See also
.sp
scipy.misc.derivative
.SH DOCUMENTATION AND CODE
.sp
Numdifftools works on Python 2.7+ and Python 3.0+.
.sp
Official releases available at: \fI\%http://pypi.python.org/pypi/numdifftools\fP \fI\%pkg_img\fP
.sp
Official documentation available at: \fI\%http://numdifftools.readthedocs.io/en/latest/\fP \fI\%docs_img\fP
.sp
Bleeding edge: \fI\%https://github.com/pbrod/numdifftools\fP\&.
.SH INSTALLATION
.sp
If you have pip installed, then simply type:
.INDENT 0.0
.INDENT 3.5
$ pip install numdifftools
.UNINDENT
.UNINDENT
.sp
to get the lastest stable version. Using pip also has the advantage that all
requirements are automatically installed.
.SH UNIT TESTS
.sp
To test if the toolbox is working paste the following in an interactive
python session:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
import numdifftools as nd
nd.test(coverage=True, doctests=True)
.ft P
.fi
.UNINDENT
.UNINDENT
.SH ACKNOWLEDGEMENT
.sp
The \fI\%numdifftools package\fP for
\fI\%Python\fP was written by Per A. Brodtkorb
based on the adaptive numerical differentiation toolbox written in
\fI\%Matlab\fP  by John D\(aqErrico [DErrico2006]\&.
.sp
Numdifftools has as of version 0.9 been extended with some of the functionality
found in the statsmodels.tools.numdiff module written by Josef Perktold
[Perktold2014]\&.
.SH REFERENCES
.IP [DErrico2006] 5
D\(aqErrico, J. R.  (2006),
Adaptive Robust Numerical Differentiation
\fI\%http://www.mathworks.com/matlabcentral/fileexchange/13490\-adaptive\-robust\-numerical\-differentiation\fP
.IP [Perktold2014] 5
Perktold, J (2014), numdiff package
\fI\%http://statsmodels.sourceforge.net/0.6.0/_modules/statsmodels/tools/numdiff.html\fP
.SH NUMERICAL DIFFERENTIATION
.SS Introduction
.sp
The general problem of differentiation of a function typically pops up in three ways in Python.
.INDENT 0.0
.IP \(bu 2
The symbolic derivative of a function.
.IP \(bu 2
Compute numerical derivatives of a function defined only by a sequence of data points.
.IP \(bu 2
Compute numerical derivatives of a analytically supplied function.
.UNINDENT
.sp
Clearly the first member of this list is the domain of the symbolic toolbox SymPy, or some set of symbolic tools. Numerical differentiation of a function defined by data points can be achieved with the function gradient, or perhaps by differentiation of a curve fit to the data, perhaps to an interpolating spline or a least squares spline fit.
.sp
The third class of differentiation problems is where Numdifftools is valuable. This document will describe the methods used in Numdifftools and in particular the Derivative class.
.SS Numerical differentiation of a general function of one variable
.sp
Surely you recall the traditional definition of a derivative, in terms of a limit.
.sp
.ce

.ce 0
.sp
For small \delta, the limit approaches f'(x)\&. This is a one\-sided approximation for the derivative. For a fixed value of \delta, this is also known as a finite difference approximation (a forward difference.) Other approximations for the derivative are also available. We will see the origin of these approximations in the Taylor series expansion of a function f(x) around some point x_0\&.
.sp
.ce

.ce 0
.sp
Truncate the series in 2 to the first three terms, divide by \delta and rearrange yields the forward difference approximation 1:
.sp
.ce

.ce 0
.sp
When \delta is small, \delta^2 and any higher powers are vanishingly small. So we tend to ignore those higher powers, and describe the approximation in 3 as a first order approximation since the error in this approximation approaches zero at the same rate as the first power of \delta\&.  [1] The values of f''(x_0) and f'''(x_0), while unknown to us, are fixed constants as \delta varies.
.sp
Higher order approximations arise in the same fashion. The central difference 4 is a second order approximation.
.sp
.ce

.ce 0
.SS Unequally spaced finite difference rules
.sp
While most finite difference rules used to differentiate a function will use equally spaced points, this fails to be appropriate when one does not know the final spacing. Adaptive quadrature rules can succeed by subdividing each sub\-interval as necessary. But an adaptive differentiation scheme must work differently, since differentiation is a point estimate. Derivative generates a sequence of sample points that follow a log spacing away from the point in question, then it uses a single rule (generated on the fly) to estimate the desired derivative. Because the points are log spaced, the same rule applies at any scale, with only a scale factor applied.
.SS Odd and even transformations of a function
.sp
Returning to the Taylor series expansion of f(x) around some point x_0, an even function  [2] around x_0 must have all the odd order derivatives vanish at x_0\&. An odd function has all its even derivatives vanish from its expansion. Consider the derived functions f_{odd}(x) and f_{even}(x)\&.
.sp
.ce

.ce 0
.sp
.ce

.ce 0
.sp
The Taylor series expansion of f_{odd}(x) around zero has the useful property that we have killed off any even order terms, but the odd order terms are identical to f(x), as expanded around x_0\&.
.sp
.ce

.ce 0
.sp
Likewise, the Taylor series expansion of f_{even}(x) has no odd order terms or a constant term, but other even order terms that are identical to f(x)\&.
.sp
.ce

.ce 0
.sp
The point of these transformations is we can rather simply generate a higher order approximation for any odd order derivatives of f(x) by working with f_{odd}(x)\&. Even order derivatives of f(x) are similarly generated from f_{even}(x)\&. For example, a second order approximation for f'(x_0) is trivially written in 9 as a function of \delta\&.
.sp
.ce

.ce 0
.sp
We can do better rather simply, so why not? 10 shows a fourth order approximation for f'(x_0)\&.
.sp
.ce

.ce 0
.sp
Again, the next non\-zero term 11 in that expansion has a higher power of \delta on it, so we would normally ignore it since the lowest order neglected term should dominate the behavior for small \delta\&.
.sp
.ce

.ce 0
.sp
Derivative uses similar approximations for all derivatives of f up to any order. Of course, it is not always possible for evaluation of a function on both sides of a point, as central difference rules will require. In these cases, you can specify forward or backward difference rules as appropriate. You can also specify to use the complex step derivative, which we will outline in the next section.
.SS Complex step derivative
.sp
The derivation of the complex\-step derivative approximation is accomplished by replacing \delta in 2
with a complex step i h:
.sp
.ce

.ce 0
.sp
Taking only the imaginary parts of both sides gives
.sp
.ce

.ce 0
.sp
Dividing with h and rearranging yields:
.sp
.ce

.ce 0
.sp
Terms with order h^2 or higher can safely be ignored since the interval h can be chosen up to machine precision
without fear of rounding errors stemming from subtraction (since there are not any). Thus to within second\-order the complex\-step derivative approximation is given by:
.sp
.ce

.ce 0
.sp
Next, consider replacing the step \delta in 8 with the complex step i^\frac{1}{2}  h:
.sp
.ce

.ce 0
.sp
Similarly dividing with h^2/2 and taking only the imaginary components yields:
.sp
.ce

.ce 0
.sp
This approximation is still subject to difference errors, but the error associated with this approximation is proportional to
h^4\&. Neglecting these higher order terms yields:
.sp
.ce

.ce 0
.sp
See [LaiCrassidisCheng2005] and [Ridout2009] for more details.
The complex\-step derivative in numdifftools.Derivative has truncation error
O(\delta^4) for both odd and even order derivatives for n>1\&. For n=1
the truncation error is on the order of O(\delta^2), so
truncation error can be eliminated by choosing steps to be very small.  The first order complex\-step derivative avoids the problem of
round\-off error with small steps because there is no subtraction. However,
the function to differentiate needs to be analytic. This method does not work if it does
not support complex numbers or involves non\-analytic functions such as
e.g.: abs, max, min. For this reason the \fIcentral\fP method is the default method.
.SS High order derivative
.sp
So how do we construct these higher order approximation formulas? Here we will deomonstrate the principle by computing the 6\(aqth order central approximation for the first\-order derivative. In order to do so we simply set f_{odd}(\delta) equal to its 3\-term Taylor expansion:
.sp
.ce

.ce 0
.sp
By inserting three different stepsizes into 12, eg \delta, \delta/2, \delta/4, we get a set of linear equations:
.sp
.ce

.ce 0
.sp
The solution of these equations are simply:
.sp
.ce

.ce 0
.sp
The first row of 14a gives the coefficients for 6\(aqth order approximation. Looking at at row two and three, we see also that
this gives the 6\(aqth order approximation for the 3\(aqrd and 5\(aqth order derivatives as bonus. Thus this is also a general method for obtaining high order differentiation rules. As previously noted these formulas have the additional benefit of beeing applicable to any scale, with only a scale factor applied.
.SS Richardson extrapolation methodology applied to derivative estimation
.sp
Some individuals might suggest that the above set of approximations are entirely adequate for any sane person. Can we do better?
.sp
Suppose we were to generate several different estimates of the approximation in 3 for different values of \delta at a fixed x_0\&. Thus, choose a single \delta, estimate a corresponding resulting approximation to f'(x_0), then do the same for \delta/2\&. If we assume that the error drops off linearly as \delta \to 0, then it is a simple matter to extrapolate this process to a zero step size. Our lack of knowledge of f''(x_0) is irrelevant. All that matters is \delta is small enough that the linear term dominates so we can ignore the quadratic term, therefore the error is purely linear.
.sp
.ce

.ce 0
.sp
The linear extrapolant for this interval halving scheme as \delta \to 0 is given by:
.sp
.ce

.ce 0
.sp
Since I\(aqve always been a big fan of convincing myself that something will work before I proceed too far, lets try this out in Python. Consider the function e^x\&. Generate a pair of approximations to f'(0), once at \delta of 0.1, and the second approximation at 1/2 that value. Recall that \frac{d(e^x)}{dx} = e^x, so at x = 0, the derivative should be exactly 1. How well will we do?
.sp
.nf
.ft C
>>> from numpy import exp, allclose
>>> f = exp
>>> dx = 0.1
>>> df1 = (f(dx) \- f(0))/dx
>>> allclose(df1, 1.05170918075648)
True
.ft P
.fi
.sp
.nf
.ft C
>>> df2 = (f(dx/2) \- f(0))/(dx/2)
>>> allclose(df2, 1.02542192752048)
True
.ft P
.fi
.sp
.nf
.ft C
>>> allclose(2*df2 \- df1, 0.999134674284488)
True
.ft P
.fi
.sp
In fact, this worked very nicely, reducing the error to roughly 1 percent of our initial estimates. Should we be surprised at this reduction? Not if we recall that last term in 3\&. We saw there that the next term in the expansion was O(\delta^2)\&. Since \delta was 0.1 in our experiment, that 1 percent number makes perfect sense.
.sp
The Richardson extrapolant in 16 assumed a linear process, with a specific reduction in \delta by a factor of 2. Assume the two term (linear + quadratic) residual term in 3, evaluating our approximation there with a third value of \delta\&. Again, assume the step size is cut in half again. The three term Richardson extrapolant is given by:
.sp
.ce

.ce 0
.sp
A quick test in Python yields much better results yet.
.sp
.nf
.ft C
>>> from numpy import exp, allclose
>>> f = exp
>>> dx = 0.1
.ft P
.fi
.sp
.nf
.ft C
>>> df1 = (f(dx) \- f(0))/dx
>>> allclose(df1,  1.05170918075648)
True
.ft P
.fi
.sp
.nf
.ft C
>>> df2 = (f(dx/2) \- f(0))/(dx/2)
>>> allclose(df2, 1.02542192752048)
True
.ft P
.fi
.sp
.nf
.ft C
>>> df3 = (f(dx/4) \- f(0))/(dx/4)
>>> allclose(df3, 1.01260482097715)
True
.ft P
.fi
.sp
.nf
.ft C
>>> allclose(1./3*df1 \- 2*df2 + 8./3*df3, 1.00000539448361)
True
.ft P
.fi
.sp
Again, Derivative uses the appropriate multiple term Richardson extrapolants for all derivatives of f up to any order [3]\&. This, combined with the use of high order approximations for the derivatives, allows the use of quite large step sizes. See [LynessMoler1966] and [LynessMoler1969]\&. How to compute the multiple term Richardson extrapolants will be elaborated further in the next section.
.SS Multiple term Richardson extrapolants
.sp
We shall now indicate how we can calculate the multiple term Richardson extrapolant for f_{odd}(\delta)/\delta by rearranging 12:
.sp
.ce

.ce 0
.sp
This equation has the form
.sp
.ce

.ce 0
.sp
where L stands for f'(x_0) and \phi(\delta) for the numerical differentiation formula f_{odd}(\delta)/\delta\&.
.sp
By neglecting higher order terms (a_3 \delta^8) and inserting three different stepsizes into 18, eg \delta, \delta/2, \delta/4, we get a set of linear equations:
.sp
.ce

.ce 0
.sp
The solution of these equations are simply:
.sp
.ce

.ce 0
.sp
The first row of 20 gives the coefficients for Richardson extrapolation scheme.
.SS Uncertainty estimates for Derivative
.sp
We can view the Richardson extrapolation step as a polynomial curve fit in the step size parameter \delta\&. Our desired extrapolated value is seen as simply the constant term coefficient in that polynomial model. Remember though, this polynomial model (see 10 and 11) has only a few terms in it with known non\-zero coefficients. That is, we will expect a constant term a_0, a term of the form a_1 \delta^4, and a third term a_2 \delta^6\&.
.sp
A neat trick to compute the statistical uncertainty in the estimate of our desired derivative is to use statistical methodology for that error estimate. While I do appreciate that there is nothing truly statistical or stochastic in this estimate, the approach still works nicely, providing a very reasonable estimate in practice. A three term Richardson\-like extrapolant, then evaluated at four distinct values for \delta, will yield an estimate of the standard error of the constant term, with one spare degree of freedom. The uncertainty is then derived by multiplying that standard error by the appropriate percentile from the Students\-t distribution.
.sp
.nf
.ft C
>>> import scipy.stats as ss
>>> allclose(ss.t.cdf(12.7062047361747, 1), 0.975)
True
.ft P
.fi
.sp
This critical level will yield a two\-sided confidence interval of 95 percent.
.sp
These error estimates are also of value in a different sense. Since they are efficiently generated at all the different scales, the particular spacing which yields the minimum predicted error is chosen as the best derivative estimate. This has been shown to work consistently well. A spacing too large tends to have large errors of approximation due to the finite difference schemes used. But a too small spacing is bad also, in that we see a significant amplification of least significant fit errors in the approximation. A middle value generally seems to yield quite good results. For example, Derivative will estimate the derivative of e^x automatically. As we see, the final overall spacing used was 0.0078125.
.sp
.nf
.ft C
>>> import numdifftools as nd
>>> from numpy import exp, allclose
>>> f = nd.Derivative(exp, full_output=True)
>>> val, info = f(1)
>>> allclose(val, 2.71828183)
True
>>> allclose(info.error_estimate, 6.927791673660977e\-14)
True
>>> allclose(info.final_step, 0.0078125)
True
.ft P
.fi
.sp
However, if we force the step size to be artificially large, then approximation error takes over.
.sp
.nf
.ft C
>>> f = nd.Derivative(exp, step=1, full_output=True)
>>> val, info = f(1)
>>> allclose(val, 3.19452805)
True
>>> allclose(val\-exp(1), 0.47624622)
True
>>> allclose(info.final_step, 1)
True
.ft P
.fi
.sp
And if the step size is forced to be too small, then we see noise dominate the problem.
.sp
.nf
.ft C
>>> f = nd.Derivative(exp, step=1e\-10, full_output=True)
>>> val, info = f(1)
>>> allclose(val, 2.71828093)
True
>>> allclose(val \- exp(1), \-8.97648138e\-07)
True
>>> allclose(info.final_step, 1.0000000e\-10)
True
.ft P
.fi
.sp
Numdifftools, like Goldilocks in the fairy tale bearing her name, stays comfortably in the middle ground.
.SS Derivative in action
.sp
How does numdifftools.Derivative work in action? A simple nonlinear function with a well known derivative is e^x\&. At x = 0, the derivative should be 1.
.sp
.nf
.ft C
>>> f = nd.Derivative(exp, full_output=True)
>>> val, info = f(0)
>>> allclose(val, 1)
True
.ft P
.fi
.sp
.nf
.ft C
>>> allclose(info.error_estimate, 5.28466160e\-14)
True
.ft P
.fi
.sp
A second simple example comes from trig functions. The first four derivatives of the sine function, evaluated at x = 0, should be respectively [cos(0), -sin(0), -cos(0), sin(0)], or [1,0,-1,0]\&.
.sp
.nf
.ft C
>>> from numpy import sin, allclose
>>> import numdifftools as nd
>>> df = nd.Derivative(sin, n=1)
>>> allclose(df(0), 1.)
True
.ft P
.fi
.sp
.nf
.ft C
>>> ddf = nd.Derivative(sin, n=2)
>>> allclose(ddf(0), 0.)
True
.ft P
.fi
.sp
.nf
.ft C
>>> dddf = nd.Derivative(sin, n=3)
>>> allclose(dddf(0), \-1.)
True
.ft P
.fi
.sp
.nf
.ft C
>>> ddddf = nd.Derivative(sin, n=4)
>>> allclose(ddddf(0), 0.)
True
.ft P
.fi
.SS Gradient and Hessian  estimation
.sp
Estimation of the gradient vector (numdifftools.Gradient) of a function of multiple variables is a simple task, requiring merely repeated calls to numdifftools.Derivative. Likewise, the diagonal elements of the hessian matrix are merely pure second partial derivatives of a function. numdifftools.Hessdiag accomplishes this task, again calling numdifftools.Derivative multiple times. Efficient computation of the off\-diagonal (mixed partial derivative) elements of the Hessian matrix uses a scheme much like that of numdifftools.Derivative, then Richardson extrapolation is used to improve a set of second order finite difference estimates of those mixed partials.
.SS Multivariate calculus examples
.sp
Typical usage of the gradient and Hessian might be in optimization problems, where one might compare
an analytically derived gradient for correctness, or use the Hessian matrix to compute confidence interval estimates on parameters in a maximum likelihood estimation.
.SS Gradients and Hessians
.sp
.nf
.ft C
>>> import numpy as np
>>> def rosen(x): return (1\-x[0])**2 + 105.*(x[1]\-x[0]**2)**2
.ft P
.fi
.INDENT 0.0
.TP
.B Gradient of the Rosenbrock function at [1,1], the global minimizer
.sp
.nf
.ft C
>>> grad = nd.Gradient(rosen)([1, 1])
.ft P
.fi
.UNINDENT
.sp
The gradient should be zero (within floating point noise)
.sp
.nf
.ft C
>>> allclose(grad, 0)
True
.ft P
.fi
.INDENT 0.0
.TP
.B The Hessian matrix at the minimizer should be positive definite
.sp
.nf
.ft C
>>> H = nd.Hessian(rosen)([1, 1])
.ft P
.fi
.UNINDENT
.sp
The eigenvalues of H should be positive
.sp
.nf
.ft C
>>> li, U = np.linalg.eig(H)
>>> li>0
array([ True,  True], dtype=bool)
.ft P
.fi
.INDENT 0.0
.TP
.B Gradient estimation of a function of 5 variables
.sp
.nf
.ft C
>>> f = lambda x: np.sum(x**2)
>>> grad = nd.Gradient(f)(np.r_[1, 2, 3, 4, 5])
>>> allclose(grad, [  2.,   4.,   6.,   8.,  10.])
True
.ft P
.fi
.TP
.B Simple Hessian matrix of a problem with 3 independent variables
.sp
.nf
.ft C
>>> f = lambda x: x[0] + x[1]**2 + x[2]**3
>>> H = nd.Hessian(f)([1, 2, 3])
>>> allclose(H, np.diag([0, 2, 18]))
True
.ft P
.fi
.TP
.B A semi\-definite Hessian matrix
.sp
.nf
.ft C
>>> H = nd.Hessian(lambda xy: np.cos(xy[0] \- xy[1]))([0, 0])
.ft P
.fi
.UNINDENT
.sp
one of these eigenvalues will be zero (approximately)
.sp
.nf
.ft C
>>> np.abs(np.linalg.eig(H)[0]) < 1e\-12
array([ True, False], dtype=bool)
.ft P
.fi
.SS Directional derivatives
.sp
The directional derivative will be the dot product of the gradient with the (unit normalized) vector. This is of course possible to do with numdifftools and you could do it like this for the Rosenbrock function at the solution, x0 = [1,1]:
.sp
.nf
.ft C
>>> v = np.r_[1, 2]/np.sqrt(5)
>>> x0 = [1, 1]
>>> directional_diff = np.dot(nd.Gradient(rosen)(x0), v)
.ft P
.fi
.sp
This should be zero.
.sp
.nf
.ft C
>>> allclose(directional_diff, 0)
True
.ft P
.fi
.sp
Ok, its a trivial test case, but it easy to compute the directional derivative at other locations:
.sp
.nf
.ft C
>>> v2 = np.r_[1, \-1]/np.sqrt(2)
>>> x2 = [2, 3]
>>> directionaldiff = np.dot(nd.Gradient(rosen)(x2), v2)
>>> allclose(directionaldiff, 743.87633380824832)
True
.ft P
.fi
.SS Jacobian matrix
.sp
Jacobian matrix of a scalar function is just the gradient
.sp
.nf
.ft C
>>> jac = nd.Jacobian(rosen)([2, 3])
>>> grad = nd.Gradient(rosen)([2, 3])
>>> allclose(jac, grad)
True
.ft P
.fi
.sp
Jacobian matrix of a linear system will reduce to the design matrix
.sp
.nf
.ft C
>>> A = np.random.rand(5,3)
>>> b = np.random.rand(5)
>>> fun = lambda x: np.dot(x, A.T) \- b
>>> x = np.random.rand(3)
>>> jac = nd.Jacobian(fun)(x)
.ft P
.fi
.sp
This should be essentially zero at any location x
.sp
.nf
.ft C
>>> allclose(jac \- A, 0)
True
.ft P
.fi
.sp
The jacobian matrix of a nonlinear transformation of variables evaluated at some
arbitrary location [\-2, \-3]
.sp
.nf
.ft C
>>> fun = lambda xy: np.r_[xy[0]**2, np.cos(xy[0] \- xy[1])]
>>> jac = nd.Jacobian(fun)([\-2, \-3])
>>> np.allclose(jac, [[\-4.,  0.],
\&...                   [\-0.84147098,  0.84147098]])
True
.ft P
.fi
.SS Conclusion
.sp
numdifftools.Derivative is an a adaptive scheme that can compute the derivative of arbitrary (well behaved) functions. It is reasonably fast as an adaptive method. Many options have been provided for the user who wishes the ultimate amount of control over the estimation.
.SS Acknowledgments
.sp
The numdifftools package was originally a translation of an adaptive numerical differentiation toolbox written in Matlab by John D\(aqErrico [DErrico2006]\&.
.sp
Numdifftools has as of version 0.9 been extended with some of the functionality
found in the statsmodels.tools.numdiff module written by Josef Perktold [Perktold2014]\&.
.SS References
.IP [LynessMoler1966] 5
Lyness, J. M., Moler, C. B. (1966). Vandermonde Systems and Numerical
Differentiation. \fINumerische Mathematik\fP\&.
.IP [LynessMoler1969] 5
Lyness, J. M., Moler, C. B. (1969). Generalized Romberg Methods for
Integrals of Derivatives. \fINumerische Mathematik\fP\&.
.IP [DErrico2006] 5
D\(aqErrico, J. R.  (2006), Adaptive Robust Numerical Differentiation
\fI\%http://www.mathworks.com/matlabcentral/fileexchange/13490\-adaptive\-robust\-numerical\-differentiation\fP
.IP [Perktold2014] 5
Perktold, J (2014), numdiff package
\fI\%http://statsmodels.sourceforge.net/0.6.0/_modules/statsmodels/tools/numdiff.html\fP
.IP [LaiCrassidisCheng2005] 5
K.\-L. Lai, J.L. Crassidis, Y. Cheng, J. Kim (2005), New complex step derivative approximations with                                                                          application to second\-order kalman filtering,
AIAA Guidance, \fINavigation and Control Conference\fP,
San Francisco, California, August 2005, AIAA\-2005\-5944.
.IP [Ridout2009] 5
Ridout, M.S. (2009) Statistical applications of the complex\-step method
of numerical differentiation. \fIThe American Statistician\fP, 63, 66\-74
.IP [NAG] 5
\fINAG Library\fP\&. NAG Fortran Library Document: D04AAF
.SH FOOTNOTES
.IP [1] 5
We would normally write these additional terms using O() notation,
where all that matters is that the error term is O(\delta) or
perhaps O(\delta^2), but explicit understanding of these
error terms will be useful in the Richardson extrapolation step later
on.
.IP [2] 5
An even function is one which expresses an even symmetry around a
given point. An even symmetry has the property that
f(x) = f(-x)\&. Likewise, an odd function expresses an odd
symmetry, wherein f(x) = -f(-x)\&.
.IP [3] 5
For practical purposes the maximum order of the derivative is between 4 and 10
depending on the function to differentiate and also the method used
in the approximation.
.SH ALGORITHMIC DIFFERENTIATION
.SS Numdifftools.nd_algopy
.sp
This module provide an easy to use interface to derivatives calculated with
AlgoPy. Algopy stands for Algorithmic Differentiation in Python.
.sp
The purpose of AlgoPy is the evaluation of higher\-order derivatives in the
forward and reverse mode of Algorithmic Differentiation (AD) of functions that
are implemented as Python programs. Particular focus are functions that contain
numerical linear algebra functions as they often appear in statistically
motivated functions. The intended use of AlgoPy is for easy prototyping at
reasonable execution speeds. More precisely, for a typical program a
directional derivative takes order 10 times as much time as time as the
function evaluation. This is approximately also true for the gradient.
.SS Algoritmic differentiation
.sp
Algorithmic differentiation (AD) is a set of techniques to numerically
evaluate the derivative of a function specified by a computer program. AD
exploits the fact that every computer program, no matter how complicated,
executes a sequence of elementary arithmetic operations (addition,
subtraction, multiplication, division, etc.) and elementary functions
(exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these
operations, derivatives of arbitrary order can be computed automatically,
accurately to working precision, and using at most a small constant factor
more arithmetic operations than the original program.
.sp
Algorithmic differentiation is not:
.sp
Symbolic differentiation, nor Numerical differentiation (the method of
finite differences). These classical methods run into problems:
symbolic differentiation leads to inefficient code (unless carefully done)
and faces the difficulty of converting a computer program into a single
expression, while numerical differentiation can introduce round\-off errors
in the discretization process and cancellation. Both classical methods have
problems with calculating higher derivatives, where the complexity and
errors increase. Finally, both classical methods are slow at computing the
partial derivatives of a function with respect to many inputs, as is needed
for gradient\-based optimization algorithms. Algoritmic differentiation
solves all of these problems.
.SS Reference
.sp
Sebastian F. Walter and Lutz Lehmann 2013,
"Algorithmic differentiation in Python with AlgoPy",
in Journal of Computational Science, vol 4, no 5, pp 334 \- 344,
\fI\%http://www.sciencedirect.com/science/article/pii/S1877750311001013\fP
.sp
\fI\%https://en.wikipedia.org/wiki/Automatic_differentiation\fP
.sp
\fI\%https://pythonhosted.org/algopy/index.html\fP
.SH LICENSE
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
Copyright (c) 2014, Per A. Brodtkorb, John D\(aqErrico
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution.

* Neither the name of the {organization} nor the names of its
  contributors may be used to endorse or promote products derived from
  this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
.ft P
.fi
.UNINDENT
.UNINDENT
.SH DEVELOPERS
.INDENT 0.0
.IP \(bu 2
Per A. Brodtkorb <per.andreas.brodtkorb (at) gmail.com>
.IP \(bu 2
John D\(aqErrico <woodchips (at) rochester.rr.com>
.UNINDENT
.SH CHANGELOG
.sp
Created with gitcommand: git shortlog v0.9.16..v0.9.17
.SS Version 0.9.17, Sep 8, 2016
.INDENT 0.0
.TP
.B Andrew Fowlie (1):
.INDENT 7.0
.IP \(bu 2
Fix ReadTheDocs link as mentioned in #21
.UNINDENT
.TP
.B Per A Brodtkorb (79):
.INDENT 7.0
.IP \(bu 2
Added test for MinMaxStepgenerator
.IP \(bu 2
Removed obsolete docs from core.py
.IP \(bu 2
Updated appveyor.yml
.IP \(bu 2
Fixed sign in inverse matrix
.IP \(bu 2
Simplified code
.IP \(bu 2
Added appveyor badge + synchronised info.py with README.rst.
.IP \(bu 2
Removed plot in help header
.IP \(bu 2
Added Programming Language :: Python :: 3.5
.IP \(bu 2
Simplified code
.IP \(bu 2
Renamed bicomplex to Bicomplex
.IP \(bu 2
Simplified example_functions.py
.IP \(bu 2
.INDENT 2.0
.TP
.B Moved MinStepGenerator, MaxStepGeneretor and MinMaxStepGenerator to step_generators.py
.INDENT 7.0
.IP \(bu 2
Unified the step generators
.IP \(bu 2
Moved step_generator tests to test_step_generators.py
.IP \(bu 2
Major simplification of step_generators.py
.UNINDENT
.UNINDENT
.IP \(bu 2
Removed duplicated code + pep8
.IP \(bu 2
Moved fornberg_weights to fornberg.py + added taylor and derivative
.IP \(bu 2
Fixed print statement
.IP \(bu 2
Replace xrange with range
.IP \(bu 2
Added examples + made computation more robust.
.IP \(bu 2
Made \(aqbackward\(aq and alias for \(aqreverse\(aq in nd_algopy.py
.IP \(bu 2
Expanded the tests + added test_docstrings to testing.py
.IP \(bu 2
Replace string interpolation with format()
.IP \(bu 2
Removed obsolete parameter
.IP \(bu 2
Smaller start radius for Fornberg method
.IP \(bu 2
Simplified "n" and "order" properties
.IP \(bu 2
Simplified default_scale
.IP \(bu 2
Removed unecessary parenthesis and code. pep8
.IP \(bu 2
Fixed a bug in Dea + small refactorings.
.IP \(bu 2
Added test for EpsAlg
.IP \(bu 2
Avoid mutable default args and prefer static methods over instance\-meth.
.IP \(bu 2
Refactored to reduce cyclomatic complexity
.IP \(bu 2
Changed some instance methods to static methods
.IP \(bu 2
Renamed non\-pythonic variable names
.IP \(bu 2
Turned on xvfb (X Virtual Framebuffer) to imitate a display.
.IP \(bu 2
Added extra test for Jacobian
.IP \(bu 2
Replace lambda function with a def
.IP \(bu 2
Removed unused import
.IP \(bu 2
Added test for epsalg
.IP \(bu 2
Fixed test_scalar_to_vector
.IP \(bu 2
Updated test_docstrings
.UNINDENT
.UNINDENT
.SS Version 0.9.15, May 10, 2016
.INDENT 0.0
.TP
.B Cody (2):
.INDENT 7.0
.IP \(bu 2
Migrated \fI%\fP string formating
.IP \(bu 2
Migrated \fI%\fP string formating
.UNINDENT
.TP
.B Per A Brodtkorb (28):
.INDENT 7.0
.IP \(bu 2
Updated README.rst + setup.cfg
.IP \(bu 2
Replaced instance methods with static methods +pep8
.IP \(bu 2
Merge branch \(aqmaster\(aq of \fI\%https://github.com/pbrod/numdifftools\fP
.IP \(bu 2
Fixed a bug: replaced missing triple quote
.IP \(bu 2
Added depsy badge
.IP \(bu 2
added .checkignore for quantificode
.IP \(bu 2
Added .codeclimate.yml
.IP \(bu 2
Fixed failing tests
.IP \(bu 2
Changed instance methods to static methods
.IP \(bu 2
Made untyped exception handlers specific
.IP \(bu 2
Replaced local function with a static method
.IP \(bu 2
Simplified tests
.IP \(bu 2
Removed duplicated code Simplified _Derivative._get_function_name
.IP \(bu 2
exclude tests from testclimate
.IP \(bu 2
Renamed test_functions.py to example_functions.py Added test_example_functions.py
.UNINDENT
.TP
.B Per A. Brodtkorb (2):
.INDENT 7.0
.IP \(bu 2
Merge pull request #17 from pbrod/autofix/wrapped2_to3_fix
.IP \(bu 2
Merge pull request #18 from pbrod/autofix/wrapped2_to3_fix\-0
.UNINDENT
.TP
.B pbrod (17):
.INDENT 7.0
.IP \(bu 2
updated conf.py
.IP \(bu 2
added numpydoc>=0.5, sphinx_rtd_theme>=0.1.7 to setup_requires if sphinx
.IP \(bu 2
updated setup.py
.IP \(bu 2
added requirements.readthedocs.txt
.IP \(bu 2
Updated README.rst with info about how to install it using conda in an anaconda package.
.IP \(bu 2
updated conda install description
.IP \(bu 2
Fixed number of arguments so it does not differs from overridden \(aq_default_base_step\(aq method
.IP \(bu 2
Added codecov to .travis.yml
.IP \(bu 2
Attempt to remove coverage of test\-files
.IP \(bu 2
Added directionaldiff function in order to calculate directional derivatives. Fixes issue #16. Also added supporting tests and examples to the documentation.
.IP \(bu 2
Fixed isssue #19 multiple observations mishandled in Jacobian
.IP \(bu 2
Moved rosen function into numdifftools.testing.py
.IP \(bu 2
updated import of rosen function from numdifftools.testing
.IP \(bu 2
Simplified code + pep8 + added TestResidue
.IP \(bu 2
Updated readme.rst and replaced string interpolation with format()
.IP \(bu 2
Cleaned Dea class + pep8
.IP \(bu 2
Updated references for Wynn extrapolation method.
.UNINDENT
.UNINDENT
.SS Version 0.9.14, November 10, 2015
.INDENT 0.0
.TP
.B pbrod (53):
.INDENT 7.0
.IP \(bu 2
Updated documentation of setup.py
.IP \(bu 2
Updated README.rst
.IP \(bu 2
updated version
.IP \(bu 2
Added more documentation
.IP \(bu 2
Updated example
.IP \(bu 2
Added .landscape.yml     updated .coveragerc, .travis.yml
.IP \(bu 2
Added coverageall to README.rst.
.IP \(bu 2
updated docs/index.rst
.IP \(bu 2
Removed unused code and added tests/test_extrapolation.py
.IP \(bu 2
updated tests
.IP \(bu 2
Added more tests
.IP \(bu 2
Readded c_abs c_atan2
.IP \(bu 2
Removed dependence on wheel, numpydoc>=0.5 and sphinx_rtd_theme>=0.1.7 (only needed for building documentation)
.IP \(bu 2
updated conda path in .travis.yml
.IP \(bu 2
added omnia channel to .travis.yml
.IP \(bu 2
Added conda_recipe files     Filtered out warnings in limits.py
.UNINDENT
.UNINDENT
.SS Version 0.9.13, October 30, 2015
.INDENT 0.0
.TP
.B pbrod (21):
.INDENT 7.0
.IP \(bu 2
Updated README.rst and CHANGES.rst.
.IP \(bu 2
updated Limits.
.IP \(bu 2
Made it possible to differentiate complex functions and allow zero\(aqth order derivative.
.IP \(bu 2
BUG: added missing derivative order, n to Gradient, Hessian, Jacobian.
.IP \(bu 2
Made test more robust.
.IP \(bu 2
Updated structure in setup according to pyscaffold version 2.4.2.
.IP \(bu 2
Updated setup.cfg and deleted duplicate tests folder.
.IP \(bu 2
removed unused code.
.IP \(bu 2
Added appveyor.yml.
.IP \(bu 2
Added required appveyor install scripts
.IP \(bu 2
Fixed bug in appveyor.yml.
.IP \(bu 2
added wheel to requirements.txt.
.IP \(bu 2
updated appveyor.yml.
.IP \(bu 2
Removed import matplotlib.
.UNINDENT
.TP
.B Justin Lecher (1):
.INDENT 7.0
.IP \(bu 2
Fix min version for numpy.
.UNINDENT
.TP
.B kikocorreoso (1):
.INDENT 7.0
.IP \(bu 2
fix some prints on run_benchmark.py to make it work with py3
.UNINDENT
.UNINDENT
.SS Version 0.9.12, August 28, 2015
.sp
pbrod (12):
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.IP \(bu 2
Updated documentation.
.IP \(bu 2
Updated version in conf.py.
.IP \(bu 2
Updated CHANGES.rst.
.IP \(bu 2
Reimplemented outlier detection and made it more robust.
.IP \(bu 2
Added limits.py with tests.
.IP \(bu 2
Updated main tests folder.
.IP \(bu 2
Moved Richardson and dea3 to extrapolation.py.
.IP \(bu 2
Making a new release in order to upload to pypi.
.UNINDENT
.UNINDENT
.UNINDENT
.SS Version 0.9.11, August 27, 2015
.INDENT 0.0
.TP
.B pbrod (2):
.INDENT 7.0
.IP \(bu 2
Fixed sphinx\-build and updated docs.
.IP \(bu 2
Fixed issue #9 Backward differentiation method fails with additional parameters.
.UNINDENT
.UNINDENT
.SS Version 0.9.10, August 26, 2015
.INDENT 0.0
.TP
.B pbrod (7):
.INDENT 7.0
.IP \(bu 2
Fixed sphinx\-build and updated docs.
.IP \(bu 2
Added more tests to nd_algopy.
.IP \(bu 2
Dropped support for Python 2.6.
.UNINDENT
.UNINDENT
.SS Version 0.9.4, August 26, 2015
.INDENT 0.0
.TP
.B pbrod (7):
.INDENT 7.0
.IP \(bu 2
Fixed sphinx\-build and updated docs.
.UNINDENT
.UNINDENT
.SS Version 0.9.3, August 23, 2015
.INDENT 0.0
.TP
.B Paul Kienzle (1):
.INDENT 7.0
.IP \(bu 2
more useful benchmark plots.
.UNINDENT
.TP
.B pbrod (7):
.INDENT 7.0
.IP \(bu 2
Fixed bugs and updated docs.
.IP \(bu 2
Major rewrite of the easy to use interface to Algopy.
.IP \(bu 2
Added possibility to calculate n\(aqth order derivative not just for n=1 in nd_algopy.
.IP \(bu 2
Added tests to the easy to use interface to algopy.
.UNINDENT
.UNINDENT
.SS Version 0.9.2, August 20, 2015
.INDENT 0.0
.TP
.B pbrod (3):
.INDENT 7.0
.IP \(bu 2
Updated documentation
.IP \(bu 2
Added parenthesis to a call to the print function
.IP \(bu 2
Made the test less strict in order to pass the tests on Travis for python 2.6 and 3.2.
.UNINDENT
.UNINDENT
.SS Version 0.9.1, August 20,2015
.INDENT 0.0
.TP
.B Christoph Deil (1):
.INDENT 7.0
.IP \(bu 2
Fix Sphinx build
.UNINDENT
.TP
.B pbrod (47):
.INDENT 7.0
.IP \(bu 2
.INDENT 2.0
.TP
.B Total remake of numdifftools with slightly different call syntax.
.INDENT 7.0
.IP \(bu 2
Can compute derivatives of order up to 10\-14 depending on function and method used.
.IP \(bu 2
Updated documentation and tests accordingly.
.IP \(bu 2
Fixed a bug in dea3.
.IP \(bu 2
Added StepsGenerator as an replacement for the adaptive option.
.IP \(bu 2
Added bicomplex class for testing the complex step second derivative.
.IP \(bu 2
Added fornberg_weights_all for computing optimal finite difference rules in a stable way.
.IP \(bu 2
Added higher order complex step derivative methods.
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.SS Version 0.7.7, December 18, 2014
.INDENT 0.0
.TP
.B pbrod (35):
.INDENT 7.0
.IP \(bu 2
Got travis\-ci working in order to run the tests automatically.
.IP \(bu 2
Fixed bugs in Dea class.
.IP \(bu 2
Fixed better error estimate for the Hessian.
.IP \(bu 2
Fixed tests for python 2.6.
.IP \(bu 2
Adding tests as subpackage.
.IP \(bu 2
Restructerd folders of numdifftools.
.UNINDENT
.UNINDENT
.SS Version 0.7.3, December 17, 2014
.INDENT 0.0
.TP
.B pbrod (5):
.INDENT 7.0
.IP \(bu 2
Small cosmetic fixes.
.IP \(bu 2
pep8 + some refactorings.
.IP \(bu 2
Simplified code by refactoring.
.UNINDENT
.UNINDENT
.SS Version 0.6.0, February 8, 2014
.INDENT 0.0
.TP
.B pbrod (20):
.INDENT 7.0
.IP \(bu 2
Update and rename README.md to README.rst.
.IP \(bu 2
Simplified call to Derivative: removed step_fix.
.IP \(bu 2
Deleted unused code.
.IP \(bu 2
Simplified and Refactored. Now possible to choose step_num=1.
.IP \(bu 2
Changed default step_nom from max(abs(x0), 0.2) to max(log2(abs(x0)), 0.2).
.IP \(bu 2
pep8ified code and made sure that all tests pass.
.UNINDENT
.UNINDENT
.SS Version 0.5.0, January 10, 2014
.INDENT 0.0
.TP
.B pbrod (9):
.INDENT 7.0
.IP \(bu 2
Updated the examples in Gradient class and in info.py.
.IP \(bu 2
Added test for vec2mat and docstrings + cosmetic fixes.
.IP \(bu 2
Refactored code into private methods.
.IP \(bu 2
Fixed issue #7: Derivative(fun)(numpy.ones((10,5)) * 2) failed.
.IP \(bu 2
Made print statements compatible with python 3.
.UNINDENT
.UNINDENT
.SS Version 0.4.0, May 5, 2012
.INDENT 0.0
.TP
.B pbrod (1)
.INDENT 7.0
.IP \(bu 2
Fixed a bug for inf and nan values.
.UNINDENT
.UNINDENT
.SS Version 0.3.5, May 19, 2011
.INDENT 0.0
.TP
.B pbrod (1)
.INDENT 7.0
.IP \(bu 2
Fixed a bug for inf and nan values.
.UNINDENT
.UNINDENT
.SS Version 0.3.4, Feb 24, 2011
.INDENT 0.0
.TP
.B pbrod (11)
.INDENT 7.0
.IP \(bu 2
Made automatic choice for the stepsize more robust.
.IP \(bu 2
Added easy to use interface to the algopy and scientificpython modules.
.UNINDENT
.UNINDENT
.SS Version 0.3.1, May 20, 2009
.INDENT 0.0
.TP
.B pbrod (4)
.INDENT 7.0
.IP \(bu 2
First version of numdifftools published on google.code
.UNINDENT
.UNINDENT
.SH MODULES
.SS numdifftools package
.SS Step generators
.TS
center;
|l|l|.
_
T{
\fBMinStepGenerator\fP([base_step, step_ratio, ...])
T}	T{
Generates a sequence of steps
T}
_
T{
\fBMaxStepGenerator\fP([base_step, step_ratio, ...])
T}	T{
Generates a sequence of steps
T}
_
T{
\fBMinMaxStepGenerator\fP
T}	T{
T}
_
.TE
.SS numdifftools.core.MinStepGenerator
.INDENT 0.0
.TP
.B class numdifftools.core.MinStepGenerator(base_step=None, step_ratio=2.0, num_steps=None, step_nom=None, offset=0, num_extrap=0, use_exact_steps=True, check_num_steps=True, scale=None)
Generates a sequence of steps
.INDENT 7.0
.TP
.B where
steps = step_nom * base_step * step_ratio ** (i + offset)
.UNINDENT
.sp
for  i = num_steps\-1,... 1, 0.
.INDENT 7.0
.TP
.B Parameters
\fBbase_step\fP : float, array\-like, optional
.INDENT 7.0
.INDENT 3.5
Defines the minimum step, if None, the value is set to EPS**(1/scale)
.UNINDENT
.UNINDENT
.sp
\fBstep_ratio\fP : real scalar, optional, default 2
.INDENT 7.0
.INDENT 3.5
Ratio between sequential steps generated.
Note: Ratio > 1
If None then step_ratio is 2 for n=1 otherwise step_ratio is 1.6
.UNINDENT
.UNINDENT
.sp
\fBnum_steps\fP : scalar integer, optional, default  min_num_steps + num_extrap
.INDENT 7.0
.INDENT 3.5
defines number of steps generated. It should be larger than
min_num_steps = (n + order \- 1) / fact where fact is 1, 2 or 4 depending
on differentiation method used.
.UNINDENT
.UNINDENT
.sp
\fBstep_nom\fP :  default maximum(log(1+|x|), 1)
.INDENT 7.0
.INDENT 3.5
Nominal step where x is supplied at runtime through the __call__ method.
.UNINDENT
.UNINDENT
.sp
\fBoffset\fP : real scalar, optional, default 0
.INDENT 7.0
.INDENT 3.5
offset to the base step
.UNINDENT
.UNINDENT
.sp
\fBnum_extrap\fP : scalar integer
.INDENT 7.0
.INDENT 3.5
num_extrap
.UNINDENT
.UNINDENT
.sp
\fBcheck_num_steps\fP : boolean
.INDENT 7.0
.INDENT 3.5
If True make sure num_steps larger than the minimum required steps.
.UNINDENT
.UNINDENT
.sp
\fBuse_exact_steps\fP : boolean
.INDENT 7.0
.INDENT 3.5
If true make sure exact steps are generated
.UNINDENT
.UNINDENT
.sp
\fBscale\fP : real scalar, optional
.INDENT 7.0
.INDENT 3.5
scale used in base step. If not None it will override the default
computed with the default_scale function.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B __init__(base_step=None, step_ratio=2.0, num_steps=None, step_nom=None, offset=0, num_extrap=0, use_exact_steps=True, check_num_steps=True, scale=None)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP([base_step, step_ratio, num_steps, ...])
T}	T{
T}
_
.TE
.UNINDENT
.SS numdifftools.core.MaxStepGenerator
.INDENT 0.0
.TP
.B class numdifftools.core.MaxStepGenerator(base_step=2.0, step_ratio=2.0, num_steps=15, step_nom=None, offset=0, num_extrap=0, use_exact_steps=False, check_num_steps=True, scale=500)
Generates a sequence of steps
.INDENT 7.0
.TP
.B where
steps = step_nom * base_step * step_ratio ** (\-i + offset)
.UNINDENT
.sp
for  i = 0, 1, ..., num_steps\-1.
.INDENT 7.0
.TP
.B Parameters
\fBbase_step\fP : float, array\-like, default 2.0
.INDENT 7.0
.INDENT 3.5
Defines the maximum step, if None, the value is set to EPS**(1/scale)
.UNINDENT
.UNINDENT
.sp
\fBstep_ratio\fP : real scalar, optional, default 2
.INDENT 7.0
.INDENT 3.5
Ratio between sequential steps generated.
Note: Ratio > 1
If None then step_ratio is 2 for n=1 otherwise step_ratio is 1.6
.UNINDENT
.UNINDENT
.sp
\fBnum_steps\fP : scalar integer, optional, default  min_num_steps + num_extrap
.INDENT 7.0
.INDENT 3.5
defines number of steps generated. It should be larger than
min_num_steps = (n + order \- 1) / fact where fact is 1, 2 or 4 depending
on differentiation method used.
.UNINDENT
.UNINDENT
.sp
\fBstep_nom\fP :  default maximum(log(1+|x|), 1)
.INDENT 7.0
.INDENT 3.5
Nominal step where x is supplied at runtime through the __call__ method.
.UNINDENT
.UNINDENT
.sp
\fBoffset\fP : real scalar, optional, default 0
.INDENT 7.0
.INDENT 3.5
offset to the base step
.UNINDENT
.UNINDENT
.sp
\fBnum_extrap\fP : scalar integer
.INDENT 7.0
.INDENT 3.5
num_extrap
.UNINDENT
.UNINDENT
.sp
\fBcheck_num_steps\fP : boolean
.INDENT 7.0
.INDENT 3.5
If True make sure num_steps larger than the minimum required steps.
.UNINDENT
.UNINDENT
.sp
\fBuse_exact_steps\fP : boolean
.INDENT 7.0
.INDENT 3.5
If true make sure exact steps are generated
.UNINDENT
.UNINDENT
.sp
\fBscale\fP : real scalar, default 500
.INDENT 7.0
.INDENT 3.5
scale used in base step.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B __init__(base_step=2.0, step_ratio=2.0, num_steps=15, step_nom=None, offset=0, num_extrap=0, use_exact_steps=False, check_num_steps=True, scale=500)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP([base_step, step_ratio, num_steps, ...])
T}	T{
T}
_
.TE
.UNINDENT
.SS Utility functions
.TS
center;
||.
_
.TE
.SS Finite difference approximations
.TS
center;
|l|l|.
_
T{
\fBfornberg_weights_all\fP
T}	T{
T}
_
T{
\fBfornberg_weights\fP
T}	T{
T}
_
T{
\fBDerivative\fP(f[, step, method, order, n, ...])
T}	T{
Calculate n\-th derivative with finite difference approximation
T}
_
T{
\fBGradient\fP(f[, step, method, order, n, ...])
T}	T{
Calculate Gradient with finite difference approximation
T}
_
T{
\fBJacobian\fP(f[, step, method, order, n, ...])
T}	T{
Calculate Jacobian with finite difference approximation
T}
_
T{
\fBHessdiag\fP(f[, step, method, order, full_output])
T}	T{
Calculate Hessian diagonal with finite difference approximation
T}
_
T{
\fBHessian\fP(f[, step, method, order, full_output])
T}	T{
Calculate Hessian with finite difference approximation
T}
_
T{
\fBdirectionaldiff\fP(f, x0, vec, **options)
T}	T{
Return directional derivative of a function of n variables
T}
_
.TE
.SS numdifftools.core.Derivative
.INDENT 0.0
.TP
.B class numdifftools.core.Derivative(f, step=None, method=\(aqcentral\(aq, order=2, n=1, full_output=False, **step_options)
Calculate n\-th derivative with finite difference approximation
.INDENT 7.0
.TP
.B Parameters
\fBf\fP : function
.INDENT 7.0
.INDENT 3.5
function of one array f(x, \fI*args\fP, \fI**kwds\fP)
.UNINDENT
.UNINDENT
.sp
\fBstep\fP : float, array\-like or StepGenerator object, optional
.INDENT 7.0
.INDENT 3.5
Defines the spacing used in the approximation.
Default is MinStepGenerator(base_step=step, step_ratio=None,
.INDENT 0.0
.INDENT 3.5
num_extrap=0, 
.nf
**
.fi
step_options)
.UNINDENT
.UNINDENT
.sp
if step or method in in [\(aqcomplex\(aq, \(aqmulticomplex\(aq],
otherwise
.INDENT 0.0
.INDENT 3.5
MaxStepGenerator(step_ratio=None, num_extrap=14, 
.nf
**
.fi
step_options)
.UNINDENT
.UNINDENT
.sp
The results are extrapolated if the StepGenerator generate more than 3
steps.
.UNINDENT
.UNINDENT
.sp
\fBmethod\fP : {\(aqcentral\(aq, \(aqcomplex\(aq, \(aqmulticomplex\(aq, \(aqforward\(aq, \(aqbackward\(aq}
.INDENT 7.0
.INDENT 3.5
defines the method used in the approximation
.UNINDENT
.UNINDENT
.sp
\fBorder\fP : int, optional
.INDENT 7.0
.INDENT 3.5
defines the order of the error term in the Taylor approximation used.
For \(aqcentral\(aq and \(aqcomplex\(aq methods, it must be an even number.
.UNINDENT
.UNINDENT
.sp
\fBn\fP : int, optional
.INDENT 7.0
.INDENT 3.5
Order of the derivative.
.UNINDENT
.UNINDENT
.sp
\fBfull_output\fP : bool, optional
.INDENT 7.0
.INDENT 3.5
If \fIfull_output\fP is False, only the derivative is returned.
If \fIfull_output\fP is True, then (der, r) is returned \fIder\fP is the
derivative, and \fIr\fP is a Results object.
.UNINDENT
.UNINDENT
.sp
\fB**step_options:\fP
.INDENT 7.0
.INDENT 3.5
options to pass on to the XXXStepGenerator used.
.UNINDENT
.UNINDENT
.TP
.B Returns
\fBder\fP : ndarray
.INDENT 7.0
.INDENT 3.5
array of derivatives
.UNINDENT
.UNINDENT
.UNINDENT
.sp
\fBSEE ALSO:\fP
.INDENT 7.0
.INDENT 3.5
\fBGradient\fP, \fBHessian\fP
.UNINDENT
.UNINDENT
Notes
.sp
Complex methods are usually the most accurate provided the function to
differentiate is analytic. The complex\-step methods also requires fewer
steps than the other methods and can work very close to the support of
a function.
The complex\-step derivative has truncation error O(steps**2) for \fIn=1\fP and
O(steps**4) for \fIn\fP larger, so truncation error can be eliminated by
choosing steps to be very small.
Especially the first order complex\-step derivative avoids the problem of
round\-off error with small steps because there is no subtraction. However,
this method fails if f(x) does not support complex numbers or involves
non\-analytic functions such as e.g.: abs, max, min.
Central difference methods are almost as accurate and has no restriction on
type of function. For this reason the \(aqcentral\(aq method is the default
method, but sometimes one can only allow evaluation in forward or backward
direction.
.sp
For all methods one should be careful in decreasing the step size too much
due to round\-off errors.
.sp
Higher order approximation methods will generally be more accurate, but may
also suffer more from numerical problems. First order methods is usually
not recommended.
.INDENT 7.0
.TP
.B __init__(f, step=None, method=\(aqcentral\(aq, order=2, n=1, full_output=False, **step_options)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP(f[, step, method, order, n, ...])
T}	T{
T}
_
.TE
Attributes
.TS
center;
|l|l|.
_
T{
\fBn\fP
T}	T{
T}
_
.TE
.UNINDENT
.SS numdifftools.core.Gradient
.INDENT 0.0
.TP
.B class numdifftools.core.Gradient(f, step=None, method=\(aqcentral\(aq, order=2, n=1, full_output=False, **step_options)
Calculate Gradient with finite difference approximation
.INDENT 7.0
.TP
.B Parameters
\fBf\fP : function
.INDENT 7.0
.INDENT 3.5
function of one array f(x, \fI*args\fP, \fI**kwds\fP)
.UNINDENT
.UNINDENT
.sp
\fBstep\fP : float, array\-like or StepGenerator object, optional
.INDENT 7.0
.INDENT 3.5
Defines the spacing used in the approximation.
Default is MinStepGenerator(base_step=step, step_ratio=None,
.INDENT 0.0
.INDENT 3.5
num_extrap=0, 
.nf
**
.fi
step_options)
.UNINDENT
.UNINDENT
.sp
if step or method in in [\(aqcomplex\(aq, \(aqmulticomplex\(aq],
otherwise
.INDENT 0.0
.INDENT 3.5
MaxStepGenerator(step_ratio=None, num_extrap=14, 
.nf
**
.fi
step_options)
.UNINDENT
.UNINDENT
.sp
The results are extrapolated if the StepGenerator generate more than 3
steps.
.UNINDENT
.UNINDENT
.sp
\fBmethod\fP : {\(aqcentral\(aq, \(aqcomplex\(aq, \(aqmulticomplex\(aq, \(aqforward\(aq, \(aqbackward\(aq}
.INDENT 7.0
.INDENT 3.5
defines the method used in the approximation
.UNINDENT
.UNINDENT
.sp
\fBorder\fP : int, optional
.INDENT 7.0
.INDENT 3.5
defines the order of the error term in the Taylor approximation used.
For \(aqcentral\(aq and \(aqcomplex\(aq methods, it must be an even number.
.UNINDENT
.UNINDENT
.sp
\fBfull_output\fP : bool, optional
.INDENT 7.0
.INDENT 3.5
If \fIfull_output\fP is False, only the derivative is returned.
If \fIfull_output\fP is True, then (der, r) is returned \fIder\fP is the
derivative, and \fIr\fP is a Results object.
.UNINDENT
.UNINDENT
.sp
\fB**step_options:\fP
.INDENT 7.0
.INDENT 3.5
options to pass on to the XXXStepGenerator used.
.UNINDENT
.UNINDENT
.TP
.B Returns
\fBgrad\fP : array
.INDENT 7.0
.INDENT 3.5
gradient
.UNINDENT
.UNINDENT
.UNINDENT
.sp
\fBSEE ALSO:\fP
.INDENT 7.0
.INDENT 3.5
\fBDerivative\fP, \fBHessian\fP, \fBJacobian\fP
.UNINDENT
.UNINDENT
Notes
.sp
Complex methods are usually the most accurate provided the function to
differentiate is analytic. The complex\-step methods also requires fewer
steps than the other methods and can work very close to the support of
a function.
The complex\-step derivative has truncation error O(steps**2) for \fIn=1\fP and
O(steps**4) for \fIn\fP larger, so truncation error can be eliminated by
choosing steps to be very small.
Especially the first order complex\-step derivative avoids the problem of
round\-off error with small steps because there is no subtraction. However,
this method fails if f(x) does not support complex numbers or involves
non\-analytic functions such as e.g.: abs, max, min.
Central difference methods are almost as accurate and has no restriction on
type of function. For this reason the \(aqcentral\(aq method is the default
method, but sometimes one can only allow evaluation in forward or backward
direction.
.sp
For all methods one should be careful in decreasing the step size too much
due to round\-off errors.
.sp
Higher order approximation methods will generally be more accurate, but may
also suffer more from numerical problems. First order methods is usually
not recommended.
.sp
If x0 is an nxm array, then f is assumed to be a function of n*m variables.
.INDENT 7.0
.TP
.B __init__(f, step=None, method=\(aqcentral\(aq, order=2, n=1, full_output=False, **step_options)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP(f[, step, method, order, n, ...])
T}	T{
T}
_
.TE
Attributes
.TS
center;
|l|l|.
_
T{
\fBn\fP
T}	T{
T}
_
.TE
.UNINDENT
.SS numdifftools.core.Jacobian
.INDENT 0.0
.TP
.B class numdifftools.core.Jacobian(f, step=None, method=\(aqcentral\(aq, order=2, n=1, full_output=False, **step_options)
Calculate Jacobian with finite difference approximation
.INDENT 7.0
.TP
.B Parameters
\fBf\fP : function
.INDENT 7.0
.INDENT 3.5
function of one array f(x, \fI*args\fP, \fI**kwds\fP)
.UNINDENT
.UNINDENT
.sp
\fBstep\fP : float, array\-like or StepGenerator object, optional
.INDENT 7.0
.INDENT 3.5
Defines the spacing used in the approximation.
Default is MinStepGenerator(base_step=step, step_ratio=None,
.INDENT 0.0
.INDENT 3.5
num_extrap=0, 
.nf
**
.fi
step_options)
.UNINDENT
.UNINDENT
.sp
if step or method in in [\(aqcomplex\(aq, \(aqmulticomplex\(aq],
otherwise
.INDENT 0.0
.INDENT 3.5
MaxStepGenerator(step_ratio=None, num_extrap=14, 
.nf
**
.fi
step_options)
.UNINDENT
.UNINDENT
.sp
The results are extrapolated if the StepGenerator generate more than 3
steps.
.UNINDENT
.UNINDENT
.sp
\fBmethod\fP : {\(aqcentral\(aq, \(aqcomplex\(aq, \(aqmulticomplex\(aq, \(aqforward\(aq, \(aqbackward\(aq}
.INDENT 7.0
.INDENT 3.5
defines the method used in the approximation
.UNINDENT
.UNINDENT
.sp
\fBorder\fP : int, optional
.INDENT 7.0
.INDENT 3.5
defines the order of the error term in the Taylor approximation used.
For \(aqcentral\(aq and \(aqcomplex\(aq methods, it must be an even number.
.UNINDENT
.UNINDENT
.sp
\fBfull_output\fP : bool, optional
.INDENT 7.0
.INDENT 3.5
If \fIfull_output\fP is False, only the derivative is returned.
If \fIfull_output\fP is True, then (der, r) is returned \fIder\fP is the
derivative, and \fIr\fP is a Results object.
.UNINDENT
.UNINDENT
.sp
\fB**step_options:\fP
.INDENT 7.0
.INDENT 3.5
options to pass on to the XXXStepGenerator used.
.UNINDENT
.UNINDENT
.TP
.B Returns
\fBjacob\fP : array
.INDENT 7.0
.INDENT 3.5
Jacobian
.UNINDENT
.UNINDENT
.UNINDENT
.sp
\fBSEE ALSO:\fP
.INDENT 7.0
.INDENT 3.5
\fBDerivative\fP, \fBHessian\fP, \fBGradient\fP
.UNINDENT
.UNINDENT
Notes
.sp
Complex methods are usually the most accurate provided the function to
differentiate is analytic. The complex\-step methods also requires fewer
steps than the other methods and can work very close to the support of
a function.
The complex\-step derivative has truncation error O(steps**2) for \fIn=1\fP and
O(steps**4) for \fIn\fP larger, so truncation error can be eliminated by
choosing steps to be very small.
Especially the first order complex\-step derivative avoids the problem of
round\-off error with small steps because there is no subtraction. However,
this method fails if f(x) does not support complex numbers or involves
non\-analytic functions such as e.g.: abs, max, min.
Central difference methods are almost as accurate and has no restriction on
type of function. For this reason the \(aqcentral\(aq method is the default
method, but sometimes one can only allow evaluation in forward or backward
direction.
.sp
For all methods one should be careful in decreasing the step size too much
due to round\-off errors.
.sp
Higher order approximation methods will generally be more accurate, but may
also suffer more from numerical problems. First order methods is usually
not recommended.
.sp
If f returns a 1d array, it returns a Jacobian. If a 2d array is returned
by f (e.g., with a value for each observation), it returns a 3d array
with the Jacobian of each observation with shape xk x nobs x xk. I.e.,
the Jacobian of the first observation would be [:, 0, :]
.INDENT 7.0
.TP
.B __init__(f, step=None, method=\(aqcentral\(aq, order=2, n=1, full_output=False, **step_options)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP(f[, step, method, order, n, ...])
T}	T{
T}
_
.TE
Attributes
.TS
center;
|l|l|.
_
T{
\fBn\fP
T}	T{
T}
_
.TE
.UNINDENT
.SS numdifftools.core.Hessdiag
.INDENT 0.0
.TP
.B class numdifftools.core.Hessdiag(f, step=None, method=\(aqcentral\(aq, order=2, full_output=False, **step_options)
Calculate Hessian diagonal with finite difference approximation
.INDENT 7.0
.TP
.B Parameters
\fBf\fP : function
.INDENT 7.0
.INDENT 3.5
function of one array f(x, \fI*args\fP, \fI**kwds\fP)
.UNINDENT
.UNINDENT
.sp
\fBstep\fP : float, array\-like or StepGenerator object, optional
.INDENT 7.0
.INDENT 3.5
Defines the spacing used in the approximation.
Default is MinStepGenerator(base_step=step, step_ratio=None,
.INDENT 0.0
.INDENT 3.5
num_extrap=0, 
.nf
**
.fi
step_options)
.UNINDENT
.UNINDENT
.sp
if step or method in in [\(aqcomplex\(aq, \(aqmulticomplex\(aq],
otherwise
.INDENT 0.0
.INDENT 3.5
MaxStepGenerator(step_ratio=None, num_extrap=14, 
.nf
**
.fi
step_options)
.UNINDENT
.UNINDENT
.sp
The results are extrapolated if the StepGenerator generate more than 3
steps.
.UNINDENT
.UNINDENT
.sp
\fBmethod\fP : {\(aqcentral\(aq, \(aqcomplex\(aq, \(aqmulticomplex\(aq, \(aqforward\(aq, \(aqbackward\(aq}
.INDENT 7.0
.INDENT 3.5
defines the method used in the approximationorder : int, optional
defines the order of the error term in the Taylor approximation used.
For \(aqcentral\(aq and \(aqcomplex\(aq methods, it must be an even number.
.UNINDENT
.UNINDENT
.sp
\fBfull_output\fP : bool, optional
.INDENT 7.0
.INDENT 3.5
If \fIfull_output\fP is False, only the derivative is returned.
If \fIfull_output\fP is True, then (der, r) is returned \fIder\fP is the
derivative, and \fIr\fP is a Results object.
.UNINDENT
.UNINDENT
.sp
\fB**step_options:\fP
.INDENT 7.0
.INDENT 3.5
options to pass on to the XXXStepGenerator used.
.UNINDENT
.UNINDENT
.TP
.B Returns
\fBhessdiag\fP : array
.INDENT 7.0
.INDENT 3.5
hessian diagonal
.UNINDENT
.UNINDENT
.UNINDENT
.sp
\fBSEE ALSO:\fP
.INDENT 7.0
.INDENT 3.5
\fBDerivative\fP, \fBHessian\fP, \fBJacobian\fP, \fBGradient\fP
.UNINDENT
.UNINDENT
Notes
.sp
Complex methods are usually the most accurate provided the function to
differentiate is analytic. The complex\-step methods also requires fewer
steps than the other methods and can work very close to the support of
a function.
The complex\-step derivative has truncation error O(steps**2) for \fIn=1\fP and
O(steps**4) for \fIn\fP larger, so truncation error can be eliminated by
choosing steps to be very small.
Especially the first order complex\-step derivative avoids the problem of
round\-off error with small steps because there is no subtraction. However,
this method fails if f(x) does not support complex numbers or involves
non\-analytic functions such as e.g.: abs, max, min.
Central difference methods are almost as accurate and has no restriction on
type of function. For this reason the \(aqcentral\(aq method is the default
method, but sometimes one can only allow evaluation in forward or backward
direction.
.sp
For all methods one should be careful in decreasing the step size too much
due to round\-off errors.
.sp
Higher order approximation methods will generally be more accurate, but may
also suffer more from numerical problems. First order methods is usually
not recommended.
.INDENT 7.0
.TP
.B __init__(f, step=None, method=\(aqcentral\(aq, order=2, full_output=False, **step_options)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP(f[, step, method, order, full_output])
T}	T{
T}
_
.TE
Attributes
.TS
center;
|l|l|.
_
T{
\fBn\fP
T}	T{
T}
_
.TE
.UNINDENT
.SS numdifftools.core.Hessian
.INDENT 0.0
.TP
.B class numdifftools.core.Hessian(f, step=None, method=\(aqcentral\(aq, order=2, full_output=False, **step_options)
Calculate Hessian with finite difference approximation
.INDENT 7.0
.TP
.B Parameters
\fBf\fP : function
.INDENT 7.0
.INDENT 3.5
function of one array f(x, \fI*args\fP, \fI**kwds\fP)
.UNINDENT
.UNINDENT
.sp
\fBstep\fP : float, array\-like or StepGenerator object, optional
.INDENT 7.0
.INDENT 3.5
Defines the spacing used in the approximation.
Default is MinStepGenerator(base_step=step, step_ratio=None,
.INDENT 0.0
.INDENT 3.5
num_extrap=0, 
.nf
**
.fi
step_options)
.UNINDENT
.UNINDENT
.sp
if step or method in in [\(aqcomplex\(aq, \(aqmulticomplex\(aq],
otherwise
.INDENT 0.0
.INDENT 3.5
MaxStepGenerator(step_ratio=None, num_extrap=14, 
.nf
**
.fi
step_options)
.UNINDENT
.UNINDENT
.sp
The results are extrapolated if the StepGenerator generate more than 3
steps.
.UNINDENT
.UNINDENT
.sp
\fBmethod\fP : {\(aqcentral\(aq, \(aqcomplex\(aq, \(aqmulticomplex\(aq, \(aqforward\(aq, \(aqbackward\(aq}
.INDENT 7.0
.INDENT 3.5
defines the method used in the approximation
.UNINDENT
.UNINDENT
.sp
\fBfull_output\fP : bool, optional
.INDENT 7.0
.INDENT 3.5
If \fIfull_output\fP is False, only the derivative is returned.
If \fIfull_output\fP is True, then (der, r) is returned \fIder\fP is the
derivative, and \fIr\fP is a Results object.
.UNINDENT
.UNINDENT
.sp
\fB**step_options:\fP
.INDENT 7.0
.INDENT 3.5
options to pass on to the XXXStepGenerator used.
.UNINDENT
.UNINDENT
.TP
.B Returns
\fBhess\fP : ndarray
.INDENT 7.0
.INDENT 3.5
array of partial second derivatives, Hessian
.UNINDENT
.UNINDENT
.UNINDENT
.sp
\fBSEE ALSO:\fP
.INDENT 7.0
.INDENT 3.5
\fBDerivative\fP, \fI\%Hessian\fP
.UNINDENT
.UNINDENT
Notes
.sp
Complex methods are usually the most accurate provided the function to
differentiate is analytic. The complex\-step methods also requires fewer
steps than the other methods and can work very close to the support of
a function.
The complex\-step derivative has truncation error O(steps**2) for \fIn=1\fP and
O(steps**4) for \fIn\fP larger, so truncation error can be eliminated by
choosing steps to be very small.
Especially the first order complex\-step derivative avoids the problem of
round\-off error with small steps because there is no subtraction. However,
this method fails if f(x) does not support complex numbers or involves
non\-analytic functions such as e.g.: abs, max, min.
Central difference methods are almost as accurate and has no restriction on
type of function. For this reason the \(aqcentral\(aq method is the default
method, but sometimes one can only allow evaluation in forward or backward
direction.
.sp
For all methods one should be careful in decreasing the step size too much
due to round\-off errors.
.sp
Computes the Hessian according to method as:
\(aqforward\(aq 7, \(aqcentral\(aq 9 and \(aqcomplex\(aq 10:
.sp
.ce

.ce 0
.sp
.ce

.ce 0
.sp
.ce

.ce 0
.sp
where e_j is a vector with element j is one and the rest
are zero and d_j is a scalar spacing steps_j\&.
.INDENT 7.0
.TP
.B __init__(f, step=None, method=\(aqcentral\(aq, order=2, full_output=False, **step_options)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP(f[, step, method, order, full_output])
T}	T{
T}
_
.TE
.UNINDENT
.SS numdifftools.core.directionaldiff
.INDENT 0.0
.TP
.B numdifftools.core.directionaldiff(f, x0, vec, **options)
Return directional derivative of a function of n variables
.INDENT 7.0
.TP
.B Parameters
\fBf: function\fP
.INDENT 7.0
.INDENT 3.5
analytical function to differentiate.
.UNINDENT
.UNINDENT
.sp
\fBx0: array\fP
.INDENT 7.0
.INDENT 3.5
vector location at which to differentiate f. If x0 is an nxm array,
then f is assumed to be a function of n*m variables.
.UNINDENT
.UNINDENT
.sp
\fBvec: array\fP
.INDENT 7.0
.INDENT 3.5
vector defining the line along which to take the derivative. It should
be the same size as x0, but need not be a vector of unit length.
.UNINDENT
.UNINDENT
.sp
\fB**options:\fP
.INDENT 7.0
.INDENT 3.5
optional arguments to pass on to Derivative.
.UNINDENT
.UNINDENT
.TP
.B Returns
dder:  scalar
.INDENT 7.0
.INDENT 3.5
estimate of the first derivative of f in the specified direction.
.UNINDENT
.UNINDENT
.UNINDENT
.sp
\fBSEE ALSO:\fP
.INDENT 7.0
.INDENT 3.5
\fBDerivative\fP, \fBGradient\fP
.UNINDENT
.UNINDENT
.UNINDENT
.SS numdifftools.extrapolation module
.TS
center;
|l|l|.
_
T{
\fBconvolve\fP(sequence, rule, **kwds)
T}	T{
Wrapper around scipy.ndimage.convolve1d that allows complex input.
T}
_
T{
\fBDea\fP([limexp])
T}	T{
Extrapolate a slowly convergent sequence
T}
_
T{
\fBdea3\fP(v0, v1, v2[, symmetric])
T}	T{
Extrapolate a slowly convergent sequence
T}
_
T{
\fBRichardson\fP([step_ratio, step, order, num_terms])
T}	T{
Extrapolates as sequence with Richardsons method
T}
_
.TE
.SS numdifftools.extrapolation.convolve
.INDENT 0.0
.TP
.B numdifftools.extrapolation.convolve(sequence, rule, **kwds)
Wrapper around scipy.ndimage.convolve1d that allows complex input.
.UNINDENT
.SS numdifftools.extrapolation.Dea
.INDENT 0.0
.TP
.B class numdifftools.extrapolation.Dea(limexp=3)
Extrapolate a slowly convergent sequence
.sp
LIMEXP  is the maximum number of elements the
epsilon table data can contain. The epsilon table
is stored in the first (LIMEXP+2) entries of EPSTAB.
.INDENT 7.0
.TP
.B __init__(limexp=3)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP([limexp])
T}	T{
T}
_
.TE
.UNINDENT
.SS numdifftools.extrapolation.dea3
.INDENT 0.0
.TP
.B numdifftools.extrapolation.dea3(v0, v1, v2, symmetric=False)
Extrapolate a slowly convergent sequence
.INDENT 7.0
.TP
.B Parameters
\fBv0, v1, v2\fP : array\-like
.INDENT 7.0
.INDENT 3.5
3 values of a convergent sequence to extrapolate
.UNINDENT
.UNINDENT
.TP
.B Returns
\fBresult\fP : array\-like
.INDENT 7.0
.INDENT 3.5
extrapolated value
.UNINDENT
.UNINDENT
.sp
\fBabserr\fP : array\-like
.INDENT 7.0
.INDENT 3.5
absolute error estimate
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.SS numdifftools.extrapolation.Richardson
.INDENT 0.0
.TP
.B class numdifftools.extrapolation.Richardson(step_ratio=2.0, step=1, order=1, num_terms=2)
Extrapolates as sequence with Richardsons method
Notes
.sp
Suppose you have series expansion that goes like this
.sp
L = f(h) + a0 * h^p_0 + a1 * h^p_1+ a2 * h^p_2 + ...
.sp
where p_i = order + step * i  and f(h) \-> L as h \-> 0, but f(0) != L.
.sp
If we evaluate the right hand side for different stepsizes h
we can fit a polynomial to that sequence of approximations.
This is exactly what this class does.
.INDENT 7.0
.TP
.B __init__(step_ratio=2.0, step=1, order=1, num_terms=2)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP([step_ratio, step, order, num_terms])
T}	T{
T}
_
T{
\fBextrapolate\fP(sequence, steps)
T}	T{
T}
_
.TE
.UNINDENT
.SS numdifftools.limits module
.TS
center;
|l|l|.
_
T{
\fBMinStepGenerator\fP([base_step, step_ratio, ...])
T}	T{
Generates a sequence of steps
T}
_
T{
\fBLimit\fP(f[, step, method, order, full_output])
T}	T{
Compute limit of a function at a given point
T}
_
.TE
.SS numdifftools.limits.MinStepGenerator
.INDENT 0.0
.TP
.B class numdifftools.limits.MinStepGenerator(base_step=None, step_ratio=2.0, num_steps=None, step_nom=None, offset=0, num_extrap=0, use_exact_steps=True, check_num_steps=True, scale=None)
Generates a sequence of steps
.INDENT 7.0
.TP
.B where
steps = step_nom * base_step * step_ratio ** (i + offset)
.UNINDENT
.sp
for  i = num_steps\-1,... 1, 0.
.INDENT 7.0
.TP
.B Parameters
\fBbase_step\fP : float, array\-like, optional
.INDENT 7.0
.INDENT 3.5
Defines the minimum step, if None, the value is set to EPS**(1/scale)
.UNINDENT
.UNINDENT
.sp
\fBstep_ratio\fP : real scalar, optional, default 2
.INDENT 7.0
.INDENT 3.5
Ratio between sequential steps generated.
Note: Ratio > 1
If None then step_ratio is 2 for n=1 otherwise step_ratio is 1.6
.UNINDENT
.UNINDENT
.sp
\fBnum_steps\fP : scalar integer, optional, default  min_num_steps + num_extrap
.INDENT 7.0
.INDENT 3.5
defines number of steps generated. It should be larger than
min_num_steps = (n + order \- 1) / fact where fact is 1, 2 or 4 depending
on differentiation method used.
.UNINDENT
.UNINDENT
.sp
\fBstep_nom\fP :  default maximum(log(1+|x|), 1)
.INDENT 7.0
.INDENT 3.5
Nominal step where x is supplied at runtime through the __call__ method.
.UNINDENT
.UNINDENT
.sp
\fBoffset\fP : real scalar, optional, default 0
.INDENT 7.0
.INDENT 3.5
offset to the base step
.UNINDENT
.UNINDENT
.sp
\fBnum_extrap\fP : scalar integer
.INDENT 7.0
.INDENT 3.5
num_extrap
.UNINDENT
.UNINDENT
.sp
\fBcheck_num_steps\fP : boolean
.INDENT 7.0
.INDENT 3.5
If True make sure num_steps larger than the minimum required steps.
.UNINDENT
.UNINDENT
.sp
\fBuse_exact_steps\fP : boolean
.INDENT 7.0
.INDENT 3.5
If true make sure exact steps are generated
.UNINDENT
.UNINDENT
.sp
\fBscale\fP : real scalar, optional
.INDENT 7.0
.INDENT 3.5
scale used in base step. If not None it will override the default
computed with the default_scale function.
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B __init__(base_step=None, step_ratio=2.0, num_steps=None, step_nom=None, offset=0, num_extrap=0, use_exact_steps=True, check_num_steps=True, scale=None)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP([base_step, step_ratio, num_steps, ...])
T}	T{
T}
_
.TE
.UNINDENT
.SS numdifftools.limits.Limit
.INDENT 0.0
.TP
.B class numdifftools.limits.Limit(f, step=None, method=\(aqabove\(aq, order=4, full_output=False, **options)
Compute limit of a function at a given point
.INDENT 7.0
.TP
.B Parameters
\fBf\fP : callable
.INDENT 7.0
.INDENT 3.5
function f(z, \fI*args\fP, \fI**kwds\fP) to compute the limit for z\->z0.
The function, f, is assumed to return a result of the same shape and
size as its input, \fIz\fP\&.
.UNINDENT
.UNINDENT
.sp
\fBstep: float, complex, array\-like or StepGenerator object, optional\fP
.INDENT 7.0
.INDENT 3.5
Defines the spacing used in the approximation.
Default is CStepGenerator(base_step=step, 
.nf
**
.fi
options)
.UNINDENT
.UNINDENT
.sp
\fBmethod\fP : {\(aqabove\(aq, \(aqbelow\(aq}
.INDENT 7.0
.INDENT 3.5
defines if the limit is taken from \fIabove\fP or \fIbelow\fP
.UNINDENT
.UNINDENT
.sp
\fBorder: positive scalar integer, optional.\fP
.INDENT 7.0
.INDENT 3.5
defines the order of approximation used to find the specified limit.
The order must be member of [1 2 3 4 5 6 7 8]. 4 is a good compromise.
.UNINDENT
.UNINDENT
.sp
\fBfull_output: bool\fP
.INDENT 7.0
.INDENT 3.5
If true return additional info.
.UNINDENT
.UNINDENT
.sp
\fBoptions:\fP
.INDENT 7.0
.INDENT 3.5
options to pass on to CStepGenerator
.UNINDENT
.UNINDENT
.TP
.B Returns
limit_fz: array like
.INDENT 7.0
.INDENT 3.5
estimated limit of f(z) as z \-\-> z0
.UNINDENT
.UNINDENT
.sp
info:
.INDENT 7.0
.INDENT 3.5
Only given if full_output is True and contains the following:
error estimate: ndarray
.INDENT 0.0
.INDENT 3.5
95 uncertainty estimate around the limit, such that
abs(limit_fz \- lim z\->z0 f(z)) < error_estimate
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B final_step: ndarray
final step used in approximation
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B __init__(f, step=None, method=\(aqabove\(aq, order=4, full_output=False, **options)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP(f[, step, method, order, full_output])
T}	T{
T}
_
T{
\fBlimit\fP(x, *args, **kwds)
T}	T{
T}
_
.TE
.UNINDENT
.SS numdifftools.multicomplex module
.TS
center;
|l|l|.
_
T{
\fBbicomplex\fP
T}	T{
T}
_
.TE
.SS numdifftools.nd_algopy module
.TS
center;
|l|l|.
_
T{
\fBJacobian\fP(f[, n, method, full_output])
T}	T{
Calculate Jacobian with Algorithmic Differentiation method
T}
_
T{
\fBHessdiag\fP(f[, method, full_output])
T}	T{
Calculate Hessian diagonal with Algorithmic Differentiation method
T}
_
T{
\fBHessian\fP(f[, method, full_output])
T}	T{
Calculate Hessian with Algorithmic Differentiation method
T}
_
T{
\fBdirectionaldiff\fP(f, x0, vec, **options)
T}	T{
Return directional derivative of a function of n variables
T}
_
.TE
.SS numdifftools.nd_algopy.Jacobian
.INDENT 0.0
.TP
.B class numdifftools.nd_algopy.Jacobian(f, n=1, method=\(aqforward\(aq, full_output=False)
Calculate Jacobian with Algorithmic Differentiation method
.INDENT 7.0
.TP
.B Parameters
\fBf\fP : function
.INDENT 7.0
.INDENT 3.5
function of one array f(x, \fI*args\fP, \fI**kwds\fP)
.UNINDENT
.UNINDENT
.sp
\fBmethod\fP : string, optional {\(aqforward\(aq, \(aqreverse\(aq}
.INDENT 7.0
.INDENT 3.5
defines method used in the approximation
.UNINDENT
.UNINDENT
.TP
.B Returns
\fBjacob\fP : array
.INDENT 7.0
.INDENT 3.5
Jacobian
.UNINDENT
.UNINDENT
.UNINDENT
.sp
\fBSEE ALSO:\fP
.INDENT 7.0
.INDENT 3.5
\fBDerivative\fP, \fBGradient\fP, \fBHessdiag\fP, \fBHessian\fP
.UNINDENT
.UNINDENT
Notes
.sp
Algorithmic differentiation is a set of techniques to numerically
evaluate the derivative of a function specified by a computer program. AD
exploits the fact that every computer program, no matter how complicated,
executes a sequence of elementary arithmetic operations (addition,
subtraction, multiplication, division, etc.) and elementary functions
(exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these
operations, derivatives of arbitrary order can be computed automatically,
accurately to working precision, and using at most a small constant factor
more arithmetic operations than the original program.
.INDENT 7.0
.TP
.B __init__(f, n=1, method=\(aqforward\(aq, full_output=False)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP(f[, n, method, full_output])
T}	T{
T}
_
T{
\fBcomputational_graph\fP(x, *args, **kwds)
T}	T{
T}
_
.TE
Attributes
.TS
center;
|l|l|.
_
T{
\fBf\fP
T}	T{
T}
_
.TE
.UNINDENT
.SS numdifftools.nd_algopy.Hessdiag
.INDENT 0.0
.TP
.B class numdifftools.nd_algopy.Hessdiag(f, method=\(aqforward\(aq, full_output=False)
Calculate Hessian diagonal with Algorithmic Differentiation method
.INDENT 7.0
.TP
.B Parameters
\fBf\fP : function
.INDENT 7.0
.INDENT 3.5
function of one array f(x, \fI*args\fP, \fI**kwds\fP)
.UNINDENT
.UNINDENT
.sp
\fBmethod\fP : string, optional {\(aqforward\(aq, \(aqreverse\(aq}
.INDENT 7.0
.INDENT 3.5
defines method used in the approximation
.UNINDENT
.UNINDENT
.TP
.B Returns
\fBhessdiag\fP : ndarray
.INDENT 7.0
.INDENT 3.5
Hessian diagonal array of partial second order derivatives.
.UNINDENT
.UNINDENT
.UNINDENT
.sp
\fBSEE ALSO:\fP
.INDENT 7.0
.INDENT 3.5
\fBDerivative\fP, \fBGradient\fP, \fBJacobian\fP, \fBHessian\fP
.UNINDENT
.UNINDENT
Notes
.sp
Algorithmic differentiation is a set of techniques to numerically
evaluate the derivative of a function specified by a computer program. AD
exploits the fact that every computer program, no matter how complicated,
executes a sequence of elementary arithmetic operations (addition,
subtraction, multiplication, division, etc.) and elementary functions
(exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these
operations, derivatives of arbitrary order can be computed automatically,
accurately to working precision, and using at most a small constant factor
more arithmetic operations than the original program.
.INDENT 7.0
.TP
.B __init__(f, method=\(aqforward\(aq, full_output=False)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP(f[, method, full_output])
T}	T{
T}
_
T{
\fBcomputational_graph\fP(x, *args, **kwds)
T}	T{
T}
_
.TE
Attributes
.TS
center;
|l|l|.
_
T{
\fBf\fP
T}	T{
T}
_
.TE
.UNINDENT
.SS numdifftools.nd_algopy.Hessian
.INDENT 0.0
.TP
.B class numdifftools.nd_algopy.Hessian(f, method=\(aqforward\(aq, full_output=False)
Calculate Hessian with Algorithmic Differentiation method
.INDENT 7.0
.TP
.B Parameters
\fBf\fP : function
.INDENT 7.0
.INDENT 3.5
function of one array f(x, \fI*args\fP, \fI**kwds\fP)
.UNINDENT
.UNINDENT
.sp
\fBmethod\fP : string, optional {\(aqforward\(aq, \(aqreverse\(aq}
.INDENT 7.0
.INDENT 3.5
defines method used in the approximation
.UNINDENT
.UNINDENT
.TP
.B Returns
\fBhess\fP : ndarray
.INDENT 7.0
.INDENT 3.5
array of partial second derivatives, Hessian
.UNINDENT
.UNINDENT
.UNINDENT
.sp
\fBSEE ALSO:\fP
.INDENT 7.0
.INDENT 3.5
\fBDerivative\fP, \fBGradient\fP, \fBJacobian\fP, \fBHessdiag\fP
.UNINDENT
.UNINDENT
Notes
.sp
Algorithmic differentiation is a set of techniques to numerically
evaluate the derivative of a function specified by a computer program. AD
exploits the fact that every computer program, no matter how complicated,
executes a sequence of elementary arithmetic operations (addition,
subtraction, multiplication, division, etc.) and elementary functions
(exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these
operations, derivatives of arbitrary order can be computed automatically,
accurately to working precision, and using at most a small constant factor
more arithmetic operations than the original program.
.INDENT 7.0
.TP
.B __init__(f, method=\(aqforward\(aq, full_output=False)
.UNINDENT
Methods
.TS
center;
|l|l|.
_
T{
\fI\%__init__\fP(f[, method, full_output])
T}	T{
T}
_
T{
\fBcomputational_graph\fP(x, *args, **kwds)
T}	T{
T}
_
.TE
Attributes
.TS
center;
|l|l|.
_
T{
\fBf\fP
T}	T{
T}
_
.TE
.UNINDENT
.SS numdifftools.nd_algopy.directionaldiff
.INDENT 0.0
.TP
.B numdifftools.nd_algopy.directionaldiff(f, x0, vec, **options)
Return directional derivative of a function of n variables
.INDENT 7.0
.TP
.B Parameters
\fBf: function\fP
.INDENT 7.0
.INDENT 3.5
analytical function to differentiate.
.UNINDENT
.UNINDENT
.sp
\fBx0: array\fP
.INDENT 7.0
.INDENT 3.5
vector location at which to differentiate f. If x0 is an nxm array,
then fun is assumed to be a function of n*m variables.
.UNINDENT
.UNINDENT
.sp
\fBvec: array\fP
.INDENT 7.0
.INDENT 3.5
vector defining the line along which to take the derivative. It should
be the same size as x0, but need not be a vector of unit length.
.UNINDENT
.UNINDENT
.sp
\fB**options:\fP
.INDENT 7.0
.INDENT 3.5
optional arguments to pass on to Derivative.
.UNINDENT
.UNINDENT
.TP
.B Returns
dder:  scalar
.INDENT 7.0
.INDENT 3.5
estimate of the first derivative of f in the specified direction.
.UNINDENT
.UNINDENT
.UNINDENT
.sp
\fBSEE ALSO:\fP
.INDENT 7.0
.INDENT 3.5
\fBDerivative\fP, \fBGradient\fP
.UNINDENT
.UNINDENT
.UNINDENT
.SS numdifftools.run_benchmark module
.INDENT 0.0
.TP
.B class numdifftools.run_benchmark.BenchmarkFunction(n)
Bases: \fI\%object\fP
.sp
Return 0.5 * np.dot(x**2, np.dot(A,x))
.UNINDENT
.INDENT 0.0
.TP
.B numdifftools.run_benchmark.compute_gradients(gradient_funs, problem_sizes)
.UNINDENT
.INDENT 0.0
.TP
.B numdifftools.run_benchmark.compute_hessians(hessian_funs, problem_sizes)
.UNINDENT
.INDENT 0.0
.TP
.B numdifftools.run_benchmark.loglimits(data, border=0.05)
.UNINDENT
.INDENT 0.0
.TP
.B numdifftools.run_benchmark.main()
.UNINDENT
.INDENT 0.0
.TP
.B numdifftools.run_benchmark.plot_errors(error_objects, problem_sizes, symbols)
.UNINDENT
.INDENT 0.0
.TP
.B numdifftools.run_benchmark.plot_runtimes(run_time_objects, problem_sizes, symbols)
.UNINDENT
.SS Module contents
.SS Introduction to Numdifftools
.sp
\fI\%pkg_img\fP \fI\%tests_img\fP \fI\%tests2_img\fP \fI\%docs_img\fP \fI\%Code Health\fP \fI\%coverage_img\fP \fI\%versions_img\fP \fI\%depsy_img\fP
.sp
Numdifftools is a suite of tools written in \fI\%_Python\fP
to solve automatic numerical differentiation problems in one or more variables.
Finite differences are used in an adaptive manner, coupled with a Richardson
extrapolation methodology to provide a maximally accurate result.
The user can configure many options like; changing the order of the method or
the extrapolation, even allowing the user to specify whether complex\-step, central,
forward or backward differences are used.
.sp
The methods provided are:
.INDENT 0.0
.IP \(bu 2
\fBDerivative\fP: Compute the derivatives of order 1 through 10 on any scalar function.
.IP \(bu 2
\fBdirectionaldiff\fP: Compute directional derivative of a function of n variables
.IP \(bu 2
\fBGradient\fP: Compute the gradient vector of a scalar function of one or more variables.
.IP \(bu 2
\fBJacobian\fP: Compute the Jacobian matrix of a vector valued function of one or more variables.
.IP \(bu 2
\fBHessian\fP: Compute the Hessian matrix of all 2nd partial derivatives of a scalar function of one or more variables.
.IP \(bu 2
\fBHessdiag\fP: Compute only the diagonal elements of the Hessian matrix
.UNINDENT
.sp
All of these methods also produce error estimates on the result.
.sp
Numdifftools also provide an easy to use interface to derivatives calculated
with in \fI\%_AlgoPy\fP\&. Algopy stands for Algorithmic
Differentiation in Python.
The purpose of AlgoPy is the evaluation of higher\-order derivatives in the
\fIforward\fP and \fIreverse\fP mode of Algorithmic Differentiation (AD) of functions
that are implemented as Python programs.
.SS Getting Started
.sp
Visualize high order derivatives of the tanh function
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> import numpy as np
>>> import numdifftools as nd
>>> import matplotlib.pyplot as plt
>>> x = np.linspace(\-2, 2, 100)
>>> for i in range(10):
\&...    df = nd.Derivative(np.tanh, n=i)
\&...    y = df(x)
\&...    h = plt.plot(x, y/np.abs(y).max())
.ft P
.fi
.sp
plt.show()
.UNINDENT
.UNINDENT
\fI\%\fP
.sp
Compute 1\(aqst and 2\(aqnd derivative of exp(x), at x == 1:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> fd = nd.Derivative(np.exp)        # 1\(aqst derivative
>>> fdd = nd.Derivative(np.exp, n=2)  # 2\(aqnd derivative
>>> np.allclose(fd(1), 2.7182818284590424)
True
>>> np.allclose(fdd(1), 2.7182818284590424)
True
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Nonlinear least squares:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> xdata = np.reshape(np.arange(0,1,0.1),(\-1,1))
>>> ydata = 1+2*np.exp(0.75*xdata)
>>> fun = lambda c: (c[0]+c[1]*np.exp(c[2]*xdata) \- ydata)**2
>>> Jfun = nd.Jacobian(fun)
>>> np.allclose(np.abs(Jfun([1,2,0.75])), 0) # should be numerically zero
True
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Compute gradient of sum(x**2):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> fun = lambda x: np.sum(x**2)
>>> dfun = nd.Gradient(fun)
>>> dfun([1,2,3])
array([ 2.,  4.,  6.])
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Compute the same with the easy to use interface to AlgoPy:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> import numdifftools.nd_algopy as nda
>>> import numpy as np
>>> fd = nda.Derivative(np.exp)        # 1\(aqst derivative
>>> fdd = nda.Derivative(np.exp, n=2)  # 2\(aqnd derivative
>>> np.allclose(fd(1), 2.7182818284590424)
True
>>> np.allclose(fdd(1), 2.7182818284590424)
True
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Nonlinear least squares:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> xdata = np.reshape(np.arange(0,1,0.1),(\-1,1))
>>> ydata = 1+2*np.exp(0.75*xdata)
>>> fun = lambda c: (c[0]+c[1]*np.exp(c[2]*xdata) \- ydata)**2
>>> Jfun = nda.Jacobian(fun, method=\(aqreverse\(aq)
>>> np.allclose(np.abs(Jfun([1,2,0.75])), 0) # should be numerically zero
True
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Compute gradient of sum(x**2):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
>>> fun = lambda x: np.sum(x**2)
>>> dfun = nda.Gradient(fun)
>>> dfun([1,2,3])
array([ 2.,  4.,  6.])
.ft P
.fi
.UNINDENT
.UNINDENT
.SS See also
.sp
scipy.misc.derivative
.SS Documentation and code
.sp
Numdifftools works on Python 2.7+ and Python 3.0+.
.sp
Official releases available at: \fI\%http://pypi.python.org/pypi/numdifftools\fP \fI\%pkg_img\fP
.sp
Official documentation available at: \fI\%http://numdifftools.readthedocs.io/en/latest/\fP \fI\%docs_img\fP
.sp
Bleeding edge: \fI\%https://github.com/pbrod/numdifftools\fP\&.
.SS Installation
.sp
If you have pip installed, then simply type:
.INDENT 0.0
.INDENT 3.5
$ pip install numdifftools
.UNINDENT
.UNINDENT
.sp
to get the lastest stable version. Using pip also has the advantage that all
requirements are automatically installed.
.SS Unit tests
.sp
To test if the toolbox is working paste the following in an interactive
python session:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
import numdifftools as nd
nd.test(coverage=True, doctests=True)
.ft P
.fi
.UNINDENT
.UNINDENT
.SS Acknowledgement
.sp
The \fI\%numdifftools package\fP for
\fI\%Python\fP was written by Per A. Brodtkorb
based on the adaptive numerical differentiation toolbox written in
\fI\%Matlab\fP  by John D\(aqErrico [DErrico2006]\&.
.sp
Numdifftools has as of version 0.9 been extended with some of the functionality
found in the statsmodels.tools.numdiff module written by Josef Perktold
[Perktold2014]\&.
.SS References
.IP [DErrico2006] 5
D\(aqErrico, J. R.  (2006),
Adaptive Robust Numerical Differentiation
\fI\%http://www.mathworks.com/matlabcentral/fileexchange/13490\-adaptive\-robust\-numerical\-differentiation\fP
.IP [Perktold2014] 5
Perktold, J (2014), numdiff package
\fI\%http://statsmodels.sourceforge.net/0.6.0/_modules/statsmodels/tools/numdiff.html\fP
.INDENT 0.0
.IP \(bu 2
genindex
.IP \(bu 2
modindex
.IP \(bu 2
search
.UNINDENT
.SH COPYRIGHT
2009-2015, Per A Brodtkorb, John D'Errico
.\" Generated by docutils manpage writer.
.
